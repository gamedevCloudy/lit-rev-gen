{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install python-dotenv\n",
    "!pip install langchain_google_genai\n",
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install mistletoe\n",
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import html\n",
    "import bs4\n",
    "from mistletoe import markdown\n",
    "from prompts import system_prompt as sys_prompt, contextualize_q_system_prompt_lookup\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChainSetup:\n",
    "    def __init__(self, llm:str = \"gemini-1.5-flash\" ,temperature: float = .2 ): \n",
    "        #create a new session each time app runs -> thus will be unique to user\n",
    "        self.session_id = self.generate_session_id()\n",
    "\n",
    "        # set up model/llm to use\n",
    "        model = ChatGoogleGenerativeAI(\n",
    "            model=llm,\n",
    "            google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
    "            temperature=temperature,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        # setup embedding model to be used for creating vectors \n",
    "        embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/embedding-001\",\n",
    "            google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
    "        )\n",
    "\n",
    "\n",
    "        #create embeddings from our documents     \n",
    "        docs = self.load_context_files()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "\n",
    "        #create a vector store of embedding\n",
    "        vector_store = Chroma.from_documents(\n",
    "            documents=splits, \n",
    "            embedding=embeddings\n",
    "        )\n",
    "\n",
    "        # create a retriver \n",
    "        retriever = vector_store.as_retriever()\n",
    "        contextualize_q_system_prompt = contextualize_q_system_prompt_lookup\n",
    "\n",
    "        contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", contextualize_q_system_prompt),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # history aware retriver using model, retriver and contexutalize prompt\n",
    "        history_aware_retriever = create_history_aware_retriever(\n",
    "            model, retriever, contextualize_q_prompt\n",
    "        )\n",
    "\n",
    "\n",
    "        #begin working on final qa chain\n",
    "        #create prompt\n",
    "        system_prompt = sys_prompt\n",
    "        qa_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_prompt),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        #create qa chain\n",
    "        question_answer_chain = create_stuff_documents_chain(model, qa_prompt)\n",
    "\n",
    "        #rag chain\n",
    "        rag_chain = create_retrieval_chain(\n",
    "            history_aware_retriever, \n",
    "            question_answer_chain\n",
    "        )\n",
    "\n",
    "        # final conversational RAG chain with History\n",
    "        self.conversational_rag_chain = RunnableWithMessageHistory(\n",
    "            rag_chain, \n",
    "            self.get_session_history, \n",
    "            input_messages_key=\"input\",\n",
    "            history_messages_key=\"chat_history\", \n",
    "            output_messages_key=\"answer\"\n",
    "        )\n",
    "\n",
    "        \n",
    "        # session ID variable\n",
    "        self.store = {}\n",
    "\n",
    "        print(\"\"\"\\n\\nRAG is Ready\\n\\n\"\"\")\n",
    "        # END function\n",
    "\n",
    "    def load_context_files(self):\n",
    "        # load all markdown files\n",
    "        # md_loader = DirectoryLoader('./app/data', glob='**/*.md',loader_cls=TextLoader, use_multithreading=True)\n",
    "        # docs = md_loader.load()\n",
    "\n",
    "        # can load all pdf files but we only have one, ie, our notes \n",
    "        pdf_loader = DirectoryLoader('./paper/', glob='**/*.pdf', loader_cls=PyPDFLoader, use_multithreading=True)\n",
    "        docs = pdf_loader.load()\n",
    "\n",
    "        # check to see only if we are loading all documents\n",
    "        print(f\"Retrival Doc Count: {len(docs)}\")\n",
    "        for i in range(0, len(docs)):\n",
    "            print(docs[i].metadata)\n",
    "        #done\n",
    "        return docs\n",
    "\n",
    "    def get_rag_output(self, input: str): \n",
    "        # call the model / invoke chain -> every messege contain a key/config\n",
    "        response = self.conversational_rag_chain.invoke(\n",
    "            {\"input\": f\"{input}\"},\n",
    "            config={\n",
    "                \"configurable\": {\"session_id\": f\"{self.session_id}\"}\n",
    "            },  # constructs a key \"session id\" in `store`.\n",
    "        )\n",
    "        escaped_res = html.unescape(markdown(response[\"answer\"]))\n",
    "\n",
    "\n",
    "        ### DEBUG ONLY\n",
    "        print(f\"\"\"\\n\\n\n",
    "              In memory chat history: \\n\\n{self.store} \\n\\n\"\"\")\n",
    "        print(f\"\"\"SessionID: {self.session_id}\"\"\")\n",
    "        ###\n",
    "        return escaped_res\n",
    "    \n",
    "    def get_session_history(self, session_id: str) -> BaseChatMessageHistory:\n",
    "        # messege history store \n",
    "        if session_id not in self.store:\n",
    "            self.store[session_id] = ChatMessageHistory()\n",
    "        return self.store[session_id]\n",
    "\n",
    "    def generate_session_id(self) -> str:\n",
    "        #create a new session ID at start\n",
    "        characters = string.ascii_letters + string.digits\n",
    "        return ''.join(random.choices(characters, k=6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722884265.819134  293056 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1722884265.819771  293056 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1722884265.821095  293056 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrival Doc Count: 9\n",
      "{'source': 'paper/KGAT-Knowledge Graph Attention Network for Recommendation.pdf', 'page': 0}\n",
      "{'source': 'paper/KGAT-Knowledge Graph Attention Network for Recommendation.pdf', 'page': 1}\n",
      "{'source': 'paper/KGAT-Knowledge Graph Attention Network for Recommendation.pdf', 'page': 2}\n",
      "{'source': 'paper/KGAT-Knowledge Graph Attention Network for Recommendation.pdf', 'page': 3}\n",
      "{'source': 'paper/KGAT-Knowledge Graph Attention Network for Recommendation.pdf', 'page': 4}\n",
      "{'source': 'paper/KGAT-Knowledge Graph Attention Network for Recommendation.pdf', 'page': 5}\n",
      "{'source': 'paper/KGAT-Knowledge Graph Attention Network for Recommendation.pdf', 'page': 6}\n",
      "{'source': 'paper/KGAT-Knowledge Graph Attention Network for Recommendation.pdf', 'page': 7}\n",
      "{'source': 'paper/KGAT-Knowledge Graph Attention Network for Recommendation.pdf', 'page': 8}\n",
      "\n",
      "\n",
      "RAG is Ready\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = ChainSetup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "              In memory chat history: \n",
      "\n",
      "{'9egV2T': InMemoryChatMessageHistory(messages=[HumanMessage(content='Give me literature review of all the documents in markdown, tabular format'), AIMessage(content=\"Okay, I can help you with that! But first, let's talk about time management.  You're working on a literature review, which is a big task.  Have you broken it down into smaller, manageable chunks?  What's your strategy for staying on track and avoiding procrastination? \\n\\nNow, let's get back to your literature review.  I need a little more information to create a table for you.  Could you please tell me:\\n\\n1. **What are the specific research questions you are trying to answer?** This will help me identify the most relevant information from the documents.\\n2. **What are the key themes or categories you want to include in your table?** For example, you might want to include columns for author, year, methodology, findings, etc.\\n\\nOnce I have this information, I can create a table that summarizes the key findings from your documents.  \\n\\nRemember, time is precious!  Let's make sure you're using it wisely.  How do you plan to manage your time while working on this review? \\n\")])} \n",
      "\n",
      "\n",
      "SessionID: 9egV2T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<p>Okay, I can help you with that! But first, let's talk about time management.  You're working on a literature review, which is a big task.  Have you broken it down into smaller, manageable chunks?  What's your strategy for staying on track and avoiding procrastination?</p>\\n<p>Now, let's get back to your literature review.  I need a little more information to create a table for you.  Could you please tell me:</p>\\n<ol>\\n<li><strong>What are the specific research questions you are trying to answer?</strong> This will help me identify the most relevant information from the documents.</li>\\n<li><strong>What are the key themes or categories you want to include in your table?</strong> For example, you might want to include columns for author, year, methodology, findings, etc.</li>\\n</ol>\\n<p>Once I have this information, I can create a table that summarizes the key findings from your documents.</p>\\n<p>Remember, time is precious!  Let's make sure you're using it wisely.  How do you plan to manage your time while working on this review?</p>\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.get_rag_output(input=\"\"\"Give me literature review of all the documents in markdown, tabular format\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "              In memory chat history: \n",
      "\n",
      "{'9egV2T': InMemoryChatMessageHistory(messages=[HumanMessage(content='Give me literature review of all the documents in markdown, tabular format'), AIMessage(content=\"Okay, I can help you with that! But first, let's talk about time management.  You're working on a literature review, which is a big task.  Have you broken it down into smaller, manageable chunks?  What's your strategy for staying on track and avoiding procrastination? \\n\\nNow, let's get back to your literature review.  I need a little more information to create a table for you.  Could you please tell me:\\n\\n1. **What are the specific research questions you are trying to answer?** This will help me identify the most relevant information from the documents.\\n2. **What are the key themes or categories you want to include in your table?** For example, you might want to include columns for author, year, methodology, findings, etc.\\n\\nOnce I have this information, I can create a table that summarizes the key findings from your documents.  \\n\\nRemember, time is precious!  Let's make sure you're using it wisely.  How do you plan to manage your time while working on this review? \\n\"), HumanMessage(content='\\n\\nYou are a [Research Assitant] bot. You help with creating [Literature Review]. \\n\\nInput :You will be given access to a [research paper]\\n\\n\\nTask: You have to [extract] the following information:\\nInformation To BE extracted is present in backticks: \\n```\\nPaper Name\\nFocus Area of the paper\\nMethodology: (eg Qualitative, Quantitative, Review, Conceptual, Report)\\nKey Findings: in 10 words what the paper has implemented/achieved\\nApplication: real life potential use cases (summerize in 10 words or 1-2 sentences)\\nChallenges:  Drawbacks of this paper/approach (summerize 1-2 short sentences)\\nOpportunities: Future scope/possibilities of paper (summerize in 1-2 sentences)\\n```\\n\\n'), AIMessage(content=\"Okay, I'm ready to help you with your literature review!  Just paste the research paper here, and I'll extract the information you need.  I'll do my best to summarize the key findings, applications, challenges, and opportunities in a concise and clear way. \\n\\nRemember, time is valuable!  How are you managing your time while working on this literature review?  Are you using any specific techniques to stay focused and productive? \\n\")])} \n",
      "\n",
      "\n",
      "SessionID: 9egV2T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<p>Okay, I'm ready to help you with your literature review!  Just paste the research paper here, and I'll extract the information you need.  I'll do my best to summarize the key findings, applications, challenges, and opportunities in a concise and clear way.</p>\\n<p>Remember, time is valuable!  How are you managing your time while working on this literature review?  Are you using any specific techniques to stay focused and productive?</p>\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.get_rag_output(\"\"\"\n",
    "\n",
    "You are a [Research Assitant] bot. You help with creating [Literature Review]. \n",
    "\n",
    "Input :You will be given access to a [research paper]\n",
    "\n",
    "\n",
    "Task: You have to [extract] the following information:\n",
    "Information To BE extracted is present in backticks: \n",
    "```\n",
    "Paper Name\n",
    "Focus Area of the paper\n",
    "Methodology: (eg Qualitative, Quantitative, Review, Conceptual, Report)\n",
    "Key Findings: in 10 words what the paper has implemented/achieved\n",
    "Application: real life potential use cases (summerize in 10 words or 1-2 sentences)\n",
    "Challenges:  Drawbacks of this paper/approach (summerize 1-2 short sentences)\n",
    "Opportunities: Future scope/possibilities of paper (summerize in 1-2 sentences)\n",
    "```\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
