{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY=os.environ['GOOGLE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aayushchaudhary/Git/college/literature-bot/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Set proper name of the path to the paper's pdf`\n",
    "\n",
    "If loading single dcoument: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('./papers/0.pdf')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Docs\n",
    "\n",
    "`here set the directory where you have the papers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(path='./papers/', glob='**/*.pdf', loader_cls=PyPDFLoader)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is loaded page by page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S2275 Attention-based graph neural networks: a\\xa0survey  \\n1 3\\nInter-layer GATs: This kind of works usually select features beyond neural network \\nlayers with multiple feature spaces, not just local neighborhoods. Across the neural net-\\nwork layer, attention in inter-layer GATs can be regarded as an operation of cross-layer \\nfusion of different feature spaces with feature fusion attention. In this term, attention-\\nbased GNNs dynamically select features from different levels, different channels, dif-\\nferent views, or different time slices. Therefore, we further divide these methods into \\nfive sub-categories (i.e., multi-level attention (Liu et\\xa0 al. 2020; Zhang et\\xa0 al. 2022c), \\nmulti-channel (Bo et\\xa0al. 2021; Luan et\\xa0al. 2021), multi-view (Wang et\\xa0al. 2020b; Yuan \\net\\xa0al. 2021b), Spatio-temporal attention (Sankar et\\xa0al. 2018; Lu et\\xa0al. 2019), and time \\nseries attention (Zhang et\\xa0al. 2021c; Zhao et\\xa0al. 2020)). By considering temporal attrib-\\nutes, Spatio-temporal attention usually uses time, spatial attention, or both in dynamic \\ngraphs, while time-series attention needs to construct dynamic graphs from time-series \\ndata first.\\nGraph Transformers: In the past two years, Transformers (Lin et\\xa0al. 2021) have achieved \\nsuperior performance in many tasks of NLP, CV, and GRL. Graph Transformers gener -\\nalize the Transformer architecture to graph representation learning, capturing long-range \\ndependency (Ying et\\xa0 al. 2021). Different from previous methods with local attention, \\nGraph Transformers learn higher-order graph properties directly via global attention. \\nGraph Transformers have developed rapidly in the field of graph deep learning, especially \\nin the task of graph classification on small and medium-sized graphs. We further divide \\nGraph Transformers into two sub-categories, namely standard Transformers (Ying et\\xa0 al. \\n2021) and GNN Transformers (Nguyen et\\xa0al. 2019). Standard Transformers usually utilize \\nthe self-attention mechanism to all nodes of the input graph, ignoring adjacencies between \\nnodes, while GNN Transformers use the GNN layer to obtain adjacency information.Fig. 6  Classification breakdown of methods for attention-based GNNs'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[12].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split all the docs into Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split pages into 1000 word chunks with buffer/overlap of 200 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size= 1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate model embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722946400.395771   43860 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    }
   ],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=GOOGLE_API_KEY)\n",
    "vector_store = Chroma.from_documents(documents=texts, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup LLM to be used and chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Query to generate review of all papers: \n",
    "\n",
    "`Should contain all the names of your papers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\"\"\"\n",
    "You are a [Research Assitant] bot. You help with creating [Literature Review]. \n",
    "\n",
    "Input :You will be given access to a [research paper]\n",
    "\n",
    "\n",
    "Task: You have to [extract] the following information:\n",
    "Information To BE extracted is present in backticks: \n",
    "\n",
    "Paper Name\n",
    "Focus Area of the paper\n",
    "Date: month/year\n",
    "Methodology: (eg Qualitative, Quantitative, Review, Conceptual, Report)\n",
    "Key Findings: in 10 words what the paper has implemented/achieved\n",
    "Application: real life potential use cases (summerize in 10 words or 1-2 points)\n",
    "Challenges:  Drawbacks of this paper/approach (summerize 1-2 short points)\n",
    "Opportunities: Future scope/possibilities of paper (summerize in 1-2 points)\n",
    "Dataset: (dataset used in the paper if available else write none) \n",
    "\n",
    "Note: if including multiple points, insert newline character or write a unordered list in markdown wrapped as a string\n",
    "\n",
    "Output Format is inside triple backticks: \n",
    "```\n",
    "python\n",
    "[\n",
    "        \n",
    "{\n",
    "\"paper_name\": \"name of the paper 0\",\n",
    ".\n",
    ".\n",
    ".\n",
    "\"opportunities\": \"...\"\n",
    "\"datatset\": \"name of the dataset\"\n",
    "},\n",
    "{\n",
    "\"paper_name\": \"name of the paper 1\",\n",
    ".\n",
    ".\n",
    ".\n",
    "\"opportunities\": \"...\"\n",
    "\"datatset\": \"name of the dataset\"\n",
    "}\n",
    "...\n",
    "]\n",
    "\n",
    "```\n",
    "         \n",
    "Names of the papers: \n",
    "<<<\n",
    "A Survey on Graph Neural Networks for Time\n",
    "Series: Forecasting, Classification, Imputation,\n",
    "and Anomaly Detection, \n",
    "Attention Is All You Need,\n",
    "Exploring Hierarchical Structures for Recommender Systems,\n",
    "Heterogeneous Graph Attention Network, \n",
    "Hyperbolic Graph Attention Network, \n",
    "KGAT: Knowledge Graph Attention Network for Recommendation, \n",
    "Knowledge Graph Embedding Based on Graph Neural Network,\n",
    "Research on the application of Nerual Network model in knowledge graph completion technology,\n",
    "Attention based graph neural networks: a survey, \n",
    "Graph neural networks for visual question answering: a systematic review, \n",
    "A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions\n",
    ">>>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain.invoke({'query': query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result is Stored in `result` object and can be accessed using key: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\n[\\n{\\n\"paper_name\": \"A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection\",\\n\"Focus Area of the paper\": \"Graph Neural Networks for Time Series\",\\n\"Date\": \"12/2022\",\\n\"Methodology\": \"Review\",\\n\"Key Findings\": \"Surveyed GNNs for time series tasks\",\\n\"Application\": \"Forecasting, classification, imputation, anomaly detection\",\\n\"Challenges\": \"Limited data availability, model complexity\",\\n\"Opportunities\": \"Explore new architectures, improve interpretability\",\\n\"datatset\": \"None\"\\n},\\n{\\n\"paper_name\": \"Attention Is All You Need\",\\n\"Focus Area of the paper\": \"Attention Mechanism in Neural Networks\",\\n\"Date\": \"06/2017\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed Transformer architecture based on attention\",\\n\"Application\": \"Machine translation, text summarization\",\\n\"Challenges\": \"High computational cost, lack of interpretability\",\\n\"Opportunities\": \"Explore new attention mechanisms, improve efficiency\",\\n\"datatset\": \"WMT 2014 English-to-German translation task\"\\n},\\n{\\n\"paper_name\": \"Exploring Hierarchical Structures for Recommender Systems\",\\n\"Focus Area of the paper\": \"Hierarchical Structures in Recommender Systems\",\\n\"Date\": \"01/2019\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed hierarchical attention network for recommendation\",\\n\"Application\": \"Recommending products, services, content\",\\n\"Challenges\": \"Data sparsity, cold-start problem\",\\n\"Opportunities\": \"Explore new hierarchical structures, improve personalization\",\\n\"datatset\": \"MovieLens 100K, Amazon Reviews\"\\n},\\n{\\n\"paper_name\": \"Heterogeneous Graph Attention Network\",\\n\"Focus Area of the paper\": \"Heterogeneous Graph Attention Networks\",\\n\"Date\": \"08/2019\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed HGAN for heterogeneous graph learning\",\\n\"Application\": \"Node classification, link prediction\",\\n\"Challenges\": \"Handling different node types and relations\",\\n\"Opportunities\": \"Explore new attention mechanisms for heterogeneous graphs\",\\n\"datatset\": \"Amazon co-purchasing network, DBLP citation network\"\\n},\\n{\\n\"paper_name\": \"Hyperbolic Graph Attention Network\",\\n\"Focus Area of the paper\": \"Hyperbolic Graph Attention Networks\",\\n\"Date\": \"02/2020\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed HGAT for learning on hyperbolic graphs\",\\n\"Application\": \"Node classification, link prediction\",\\n\"Challenges\": \"Handling non-Euclidean space, model complexity\",\\n\"Opportunities\": \"Explore new hyperbolic embedding techniques\",\\n\"datatset\": \"Cora, PubMed, Amazon\"\\n},\\n{\\n\"paper_name\": \"KGAT: Knowledge Graph Attention Network for Recommendation\",\\n\"Focus Area of the paper\": \"Knowledge Graph Attention Networks for Recommendation\",\\n\"Date\": \"06/2019\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed KGAT for incorporating knowledge graphs into recommendation\",\\n\"Application\": \"Recommending products, services, content\",\\n\"Challenges\": \"Scalability, knowledge graph incompleteness\",\\n\"Opportunities\": \"Explore new ways to integrate knowledge graphs\",\\n\"datatset\": \"Amazon, MovieLens\"\\n},\\n{\\n\"paper_name\": \"Knowledge Graph Embedding Based on Graph Neural Network\",\\n\"Focus Area of the paper\": \"Knowledge Graph Embedding using GNNs\",\\n\"Date\": \"03/2020\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed GNN-based embedding for knowledge graphs\",\\n\"Application\": \"Knowledge graph completion, question answering\",\\n\"Challenges\": \"Data sparsity, model complexity\",\\n\"Opportunities\": \"Explore new GNN architectures for knowledge graphs\",\\n\"datatset\": \"FB15k-237, WN18RR\"\\n},\\n{\\n\"paper_name\": \"Research on the application of Nerual Network model in knowledge graph completion technology\",\\n\"Focus Area of the paper\": \"Knowledge Graph Completion using Neural Networks\",\\n\"Date\": \"06/2020\",\\n\"Methodology\": \"Review\",\\n\"Key Findings\": \"Surveyed NN models for knowledge graph completion\",\\n\"Application\": \"Knowledge graph completion, question answering\",\\n\"Challenges\": \"Data sparsity, model complexity\",\\n\"Opportunities\": \"Explore new NN architectures for knowledge graphs\",\\n\"datatset\": \"None\"\\n},\\n{\\n\"paper_name\": \"Attention based graph neural networks: a survey\",\\n\"Focus Area of the paper\": \"Attention Mechanisms in Graph Neural Networks\",\\n\"Date\": \"06/2021\",\\n\"Methodology\": \"Review\",\\n\"Key Findings\": \"Surveyed attention-based GNNs\",\\n\"Application\": \"Node classification, link prediction, graph classification\",\\n\"Challenges\": \"Computational cost, interpretability\",\\n\"Opportunities\": \"Explore new attention mechanisms, improve efficiency\",\\n\"datatset\": \"None\"\\n},\\n{\\n\"paper_name\": \"Graph neural networks for visual question answering: a systematic review\",\\n\"Focus Area of the paper\": \"Graph Neural Networks for Visual Question Answering\",\\n\"Date\": \"08/2021\",\\n\"Methodology\": \"Review\",\\n\"Key Findings\": \"Surveyed GNNs for visual question answering\",\\n\"Application\": \"Visual question answering, image captioning\",\\n\"Challenges\": \"Data scarcity, model complexity\",\\n\"Opportunities\": \"Explore new GNN architectures for visual tasks\",\\n\"datatset\": \"None\"\\n},\\n{\\n\"paper_name\": \"A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions\",\\n\"Focus Area of the paper\": \"Graph Neural Networks\",\\n\"Date\": \"12/2020\",\\n\"Methodology\": \"Review\",\\n\"Key Findings\": \"Comprehensive review of GNNs\",\\n\"Application\": \"Node classification, link prediction, graph classification\",\\n\"Challenges\": \"Scalability, interpretability, data sparsity\",\\n\"Opportunities\": \"Explore new GNN architectures, improve interpretability\",\\n\"datatset\": \"None\"\\n}\\n]\\n```'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Output is formatted in a way that it can be parsed as a python list of dicts/JSON objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "[\n",
       "{\n",
       "\"paper_name\": \"A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection\",\n",
       "\"Focus Area of the paper\": \"Graph Neural Networks for Time Series\",\n",
       "\"Date\": \"12/2022\",\n",
       "\"Methodology\": \"Review\",\n",
       "\"Key Findings\": \"Surveyed GNNs for time series tasks\",\n",
       "\"Application\": \"Forecasting, classification, imputation, anomaly detection\",\n",
       "\"Challenges\": \"Limited data availability, model complexity\",\n",
       "\"Opportunities\": \"Explore new architectures, improve interpretability\",\n",
       "\"datatset\": \"None\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Attention Is All You Need\",\n",
       "\"Focus Area of the paper\": \"Attention Mechanism in Neural Networks\",\n",
       "\"Date\": \"06/2017\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed Transformer architecture based on attention\",\n",
       "\"Application\": \"Machine translation, text summarization\",\n",
       "\"Challenges\": \"High computational cost, lack of interpretability\",\n",
       "\"Opportunities\": \"Explore new attention mechanisms, improve efficiency\",\n",
       "\"datatset\": \"WMT 2014 English-to-German translation task\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Exploring Hierarchical Structures for Recommender Systems\",\n",
       "\"Focus Area of the paper\": \"Hierarchical Structures in Recommender Systems\",\n",
       "\"Date\": \"01/2019\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed hierarchical attention network for recommendation\",\n",
       "\"Application\": \"Recommending products, services, content\",\n",
       "\"Challenges\": \"Data sparsity, cold-start problem\",\n",
       "\"Opportunities\": \"Explore new hierarchical structures, improve personalization\",\n",
       "\"datatset\": \"MovieLens 100K, Amazon Reviews\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Heterogeneous Graph Attention Network\",\n",
       "\"Focus Area of the paper\": \"Heterogeneous Graph Attention Networks\",\n",
       "\"Date\": \"08/2019\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed HGAN for heterogeneous graph learning\",\n",
       "\"Application\": \"Node classification, link prediction\",\n",
       "\"Challenges\": \"Handling different node types and relations\",\n",
       "\"Opportunities\": \"Explore new attention mechanisms for heterogeneous graphs\",\n",
       "\"datatset\": \"Amazon co-purchasing network, DBLP citation network\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Hyperbolic Graph Attention Network\",\n",
       "\"Focus Area of the paper\": \"Hyperbolic Graph Attention Networks\",\n",
       "\"Date\": \"02/2020\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed HGAT for learning on hyperbolic graphs\",\n",
       "\"Application\": \"Node classification, link prediction\",\n",
       "\"Challenges\": \"Handling non-Euclidean space, model complexity\",\n",
       "\"Opportunities\": \"Explore new hyperbolic embedding techniques\",\n",
       "\"datatset\": \"Cora, PubMed, Amazon\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"KGAT: Knowledge Graph Attention Network for Recommendation\",\n",
       "\"Focus Area of the paper\": \"Knowledge Graph Attention Networks for Recommendation\",\n",
       "\"Date\": \"06/2019\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed KGAT for incorporating knowledge graphs into recommendation\",\n",
       "\"Application\": \"Recommending products, services, content\",\n",
       "\"Challenges\": \"Scalability, knowledge graph incompleteness\",\n",
       "\"Opportunities\": \"Explore new ways to integrate knowledge graphs\",\n",
       "\"datatset\": \"Amazon, MovieLens\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Knowledge Graph Embedding Based on Graph Neural Network\",\n",
       "\"Focus Area of the paper\": \"Knowledge Graph Embedding using GNNs\",\n",
       "\"Date\": \"03/2020\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed GNN-based embedding for knowledge graphs\",\n",
       "\"Application\": \"Knowledge graph completion, question answering\",\n",
       "\"Challenges\": \"Data sparsity, model complexity\",\n",
       "\"Opportunities\": \"Explore new GNN architectures for knowledge graphs\",\n",
       "\"datatset\": \"FB15k-237, WN18RR\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Research on the application of Nerual Network model in knowledge graph completion technology\",\n",
       "\"Focus Area of the paper\": \"Knowledge Graph Completion using Neural Networks\",\n",
       "\"Date\": \"06/2020\",\n",
       "\"Methodology\": \"Review\",\n",
       "\"Key Findings\": \"Surveyed NN models for knowledge graph completion\",\n",
       "\"Application\": \"Knowledge graph completion, question answering\",\n",
       "\"Challenges\": \"Data sparsity, model complexity\",\n",
       "\"Opportunities\": \"Explore new NN architectures for knowledge graphs\",\n",
       "\"datatset\": \"None\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Attention based graph neural networks: a survey\",\n",
       "\"Focus Area of the paper\": \"Attention Mechanisms in Graph Neural Networks\",\n",
       "\"Date\": \"06/2021\",\n",
       "\"Methodology\": \"Review\",\n",
       "\"Key Findings\": \"Surveyed attention-based GNNs\",\n",
       "\"Application\": \"Node classification, link prediction, graph classification\",\n",
       "\"Challenges\": \"Computational cost, interpretability\",\n",
       "\"Opportunities\": \"Explore new attention mechanisms, improve efficiency\",\n",
       "\"datatset\": \"None\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Graph neural networks for visual question answering: a systematic review\",\n",
       "\"Focus Area of the paper\": \"Graph Neural Networks for Visual Question Answering\",\n",
       "\"Date\": \"08/2021\",\n",
       "\"Methodology\": \"Review\",\n",
       "\"Key Findings\": \"Surveyed GNNs for visual question answering\",\n",
       "\"Application\": \"Visual question answering, image captioning\",\n",
       "\"Challenges\": \"Data scarcity, model complexity\",\n",
       "\"Opportunities\": \"Explore new GNN architectures for visual tasks\",\n",
       "\"datatset\": \"None\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions\",\n",
       "\"Focus Area of the paper\": \"Graph Neural Networks\",\n",
       "\"Date\": \"12/2020\",\n",
       "\"Methodology\": \"Review\",\n",
       "\"Key Findings\": \"Comprehensive review of GNNs\",\n",
       "\"Application\": \"Node classification, link prediction, graph classification\",\n",
       "\"Challenges\": \"Scalability, interpretability, data sparsity\",\n",
       "\"Opportunities\": \"Explore new GNN architectures, improve interpretability\",\n",
       "\"datatset\": \"None\"\n",
       "}\n",
       "]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Automated Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cleaner approach to run one by one: \n",
    "- will help with limited context\n",
    "- possibly better accuracy and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_promp_template = (\"\"\"\n",
    "You are a [Research Assitant] bot. You help with creating [Literature Review]. \n",
    "\n",
    "Input :You will be given access to a [research paper]\n",
    "\n",
    "\n",
    "Task: You have to [extract] the following information:\n",
    "Information To BE extracted is present in backticks: \n",
    "\n",
    "Paper Name\n",
    "Focus Area of the paper\n",
    "Date: month/year\n",
    "Methodology: (eg Qualitative, Quantitative, Review, Conceptual, Report)\n",
    "Key Findings: in 10 words what the paper has implemented/achieved\n",
    "Application: real life potential use cases (summerize in 10 words or 1-2 points)\n",
    "Challenges:  Drawbacks of this paper/approach (summerize 1-2 short points)\n",
    "Opportunities: Future scope/possibilities of paper (summerize in 1-2 points)\n",
    "Dataset: (dataset used in the paper if available else write none) \n",
    "\n",
    "Note: if including multiple points, insert newline character or write a unordered list in markdown wrapped as a string\n",
    "\n",
    "Output Format is inside triple backticks: \n",
    "```\n",
    "json      \n",
    "{\n",
    "\"paper_name\": \"name of the paper 0\",\n",
    ".\n",
    ".\n",
    ".\n",
    "\"opportunities\": \"...\"\n",
    "\"datatset\": \"name of the dataset\"\n",
    "}\n",
    "```\n",
    "              \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_template = \"\"\"\n",
    "{system_prompt_template}\n",
    "Generate a literature review based on this research paper\n",
    "{paper_name}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "final_prompt_template= PromptTemplate.from_template(user_query_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All our processing for the paper consolidated into one function** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_paper(file_path, paper_name: str): \n",
    "    loader = PyPDFLoader(file_path)\n",
    "    data = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size= 1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(data)\n",
    "    \n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=GOOGLE_API_KEY)\n",
    "    vector_store = Chroma.from_documents(documents=texts, embedding=embeddings)\n",
    "\n",
    "    retriever = vector_store.as_retriever()\n",
    "    \n",
    "    llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\", \n",
    "        retriever=retriever,\n",
    "        return_source_documents=True    \n",
    "    )\n",
    "\n",
    "\n",
    "    final_prompt = final_prompt_template.format(\n",
    "        system_prompt_template=system_promp_template, \n",
    "        paper_name =paper_name\n",
    "    )\n",
    "    result = qa_chain.invoke({'query': final_prompt})\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A simple functions to parseout the JSON\n",
    "- Alternatively you can setup JSONOutputParser from langchain.outputparsers (reqires additional setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def extract_and_append_json(json_string):\n",
    "    # Remove the ```json and ``` markers if present\n",
    "    json_string = json_string.strip()\n",
    "    if json_string.startswith(\"```json\"):\n",
    "        json_string = json_string[7:]\n",
    "    if json_string.endswith(\"```\"):\n",
    "        json_string = json_string[:-3]\n",
    "    \n",
    "    # Parse the JSON string\n",
    "    try:\n",
    "        json_data = json.loads(json_string)\n",
    "        \n",
    "        # Append the parsed JSON to the result list\n",
    "        return json_data\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name of all the papers you're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paper_names = [\n",
    "\"Self-Supervised Learning of Graph Neural Networks\",\n",
    "\"\"\"A Survey on Graph Neural Networks for Time\n",
    "Series: Forecasting, Classification, Imputation,\n",
    "and Anomaly Detection\"\"\", \n",
    "\"\"\"Attention Is All You Need\"\"\",\n",
    "\"\"\"Exploring Hierarchical Structures for Recommender Systems\"\"\",\n",
    "\"\"\"Heterogeneous Graph Attention Network\"\"\", \n",
    "\"\"\"Hyperbolic Graph Attention Network\"\"\", \n",
    "\"\"\"KGAT: Knowledge Graph Attention Network for Recommendation\"\"\", \n",
    "\"\"\"Knowledge Graph Embedding Based on Graph Neural Network\"\"\",\n",
    "\"\"\"Research on the application of Nerual Network model in knowledge graph completion technology\"\"\",\n",
    "\"\"\"Attention based graph neural networks: a survey\"\"\", \n",
    "\"\"\"Graph neural networks for visual question answering: a systematic review\"\"\", \n",
    "\"\"\"A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions\"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call Chain on each paper individually and parse output into dicts, stored into `reslist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_directory = './papers'\n",
    "\n",
    "#stores the result\n",
    "reslist = []\n",
    "ct = 0\n",
    "\n",
    "for filename in os.listdir(papers_directory):\n",
    "    if ct > 9: break\n",
    "    if ct == 2: pass\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(papers_directory, filename)\n",
    "        print(f\"Processing: {file_path}\")\n",
    "        num = int(filename[:-4])\n",
    "\n",
    "\n",
    "        result = process_paper(file_path, paper_names[ct])\n",
    "\n",
    "        print(f\"Processed Paper: {paper_names[ct]}\")\n",
    "\n",
    "        reslist.append(extract_and_append_json(result['result']))\n",
    "    ct+=1\n",
    "\n",
    "reslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Accessing One Object```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_name': 'A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection',\n",
       " 'focus_area_of_the_paper': 'Graph Neural Networks for Time Series Analysis',\n",
       " 'date': '02/2024',\n",
       " 'methodology': 'Review',\n",
       " 'key_findings': 'Surveyed GNN applications for time series tasks',\n",
       " 'application': '- Forecasting future values\\n- Detecting anomalies in time series data',\n",
       " 'challenges': '- Limited availability of suitable time series datasets\\n- Scalability of GNNs for large-scale time series data',\n",
       " 'opportunities': '- Development of new GNN architectures for time series\\n- Integration of GNNs with other time series methods',\n",
       " 'dataset': 'None'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reslist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- Development of new GNN architectures for time series\n",
       "- Integration of GNNs with other time series methods"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(reslist[1]['opportunities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also create another chain that takes in these json lists and turn those into MD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not then just return MD instead of JSON in first place? \n",
    "- allows to process each paper individually\n",
    "- can feed into other chains easily\n",
    "- cleaner and structured approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_markdown_tempalte = \"\"\"\n",
    "Given a list of JSON objects inside triple backticks. \\\n",
    "Generate a table in [Markdown] using the keys as Table Columns\n",
    "\n",
    "```{list_of_papers}```\n",
    "\"\"\"\n",
    "\n",
    "md_prompt = PromptTemplate.from_template(create_markdown_tempalte)\n",
    "\n",
    "fin_prompt = md_prompt.format(\n",
    "    list_of_papers=reslist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_review_md = llm.invoke(fin_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| paper_name | focus_area_of_the_paper | date | methodology | key_findings | application | challenges | opportunities | dataset |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Self-Supervised Learning of Graph Neural Networks | Graph Neural Networks, Self-Supervised Learning | 01/2021 | Review | Survey of self-supervised learning for GNNs | Drug discovery, social network analysis | Lack of standardized evaluation metrics, data scarcity | Development of new self-supervised learning methods, application to real-world problems | None |\n",
       "| A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection | Graph Neural Networks for Time Series Analysis | 02/2024 | Review | Surveyed GNN applications for time series tasks | - Forecasting future values<br>- Detecting anomalies in time series data | - Limited availability of suitable time series datasets<br>- Scalability of GNNs for large-scale time series data | - Development of new GNN architectures for time series<br>- Integration of GNNs with other time series methods | None |\n",
       "| Attention Is All You Need | Machine Translation | 6/2017 | Quantitative | Proposes Transformer architecture for machine translation | Machine translation, text summarization, question answering | Computational complexity for long sequences, lack of inductive bias | Improved performance on various NLP tasks, further research on architecture variations | WMT 2014 English-to-German translation task |\n",
       "| Exploring Hierarchical Structures for Recommender Systems | Recommender Systems | 12/2019 | Review | Survey of hierarchical structures in recommender systems | Improve recommendation accuracy and efficiency | Complexity of hierarchical structures, lack of standardized evaluation metrics | Develop novel hierarchical structures, explore applications in diverse domains | None |\n",
       "| Heterogeneous Graph Attention Network | Graph Neural Networks, Heterogeneous Graphs | 12/2020 | Quantitative | Proposes a novel heterogeneous graph attention network | Recommender systems, knowledge graph completion | Scalability to large graphs,  handling complex graph structures | Further research on different attention mechanisms,  exploration of new applications | None |\n",
       "| Hyperbolic Graph Attention Network | Graph Neural Networks | 12/2020 | Quantitative | Hyperbolic space improves graph attention networks | Node classification, link prediction, graph clustering | Hyperbolic space computation complexity,  Limited scalability | Explore applications in other domains,  Improve efficiency and scalability | Cora, Citeseer, PubMed |\n",
       "| KGAT: Knowledge Graph Attention Network for Recommendation | Recommender Systems | 00/2020 | Quantitative | Knowledge graph attention network for recommendation | Personalized recommendations based on user preferences and item relationships | Overlooking relations among instances or items | Further exploration of different knowledge graph embedding techniques and attention mechanisms | Amazon-Book, Last-FM, Yelp2018 |\n",
       "| Knowledge Graph Embedding Based on Graph Neural Network | Knowledge Graph Completion | N/A | Conceptual | Proposes a new approach for KG embedding | Improve knowledge representation and reasoning | Scalability for large knowledge graphs | Integration with other machine learning techniques | None |\n",
       "| Attention-based graph neural networks: a survey | Attention-based Graph Neural Networks | 01/2023 | Review | Survey of attention-based GNN architectures | Graph-based machine learning tasks | Overfitting to specific graph structures<br>Limited scalability to large graphs | Development of more efficient and robust attention mechanisms<br>Exploration of novel applications in various domains | None |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(lit_review_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lit_review_md)\n",
    "display(Markdown(lit_review_md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDEA GENERATION - generate ideas for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_ideas_tempalte = \"\"\"\n",
    "You are a `Machine Learning` professor and we your students need to [work on final year project]. \n",
    "For that we have to come up with a list of papers to refer to. \n",
    "\n",
    "`Based on them we have to come up with some research gaps we can fulfil in certain domain or topic` with the goal of: \n",
    "`Write a Research Paper`\n",
    "and `Create the final project`. \n",
    "\n",
    "We need your help to come up with <potential areas> our team can address as final year B.E. students. \n",
    "\n",
    "The project has to build upon existing research and its helpful if we create something that is `trending` in research like LLMs and VLM etc. \n",
    "\n",
    "Our areas of Interest: \n",
    "- Knowledge Representation (very important)\n",
    "- GAT\n",
    "- Deep Learning \n",
    "- LLMs\n",
    "\n",
    "\n",
    "\n",
    "`our idea has to be novel`\n",
    "\n",
    "\n",
    "END GOAL: \n",
    "<\n",
    "- Come up with 3-5`objectives` that we can have as outcomes of our project\n",
    "- final projects that we can do (END TO END Ideas)\n",
    "- workflow or project plan for that project \n",
    "- scope of project should be 3 months\n",
    "- expose us to a wide range of experiences\n",
    "- help come up with names for the project \n",
    ">\n",
    "\n",
    "\n",
    "Given a list of papers as `Literature Review` in Markdown format.\n",
    "\n",
    "```{lit_review}```\n",
    "\n",
    "\n",
    "Output Format: \n",
    "```markdown\n",
    "# Project Title: Title of Project\n",
    "# Paper Title: Title of Paper\n",
    "# Abstract: 5-10 sentences\n",
    "# Motivation: Why do we want to build on this research (4 bullet points)\n",
    "# Objectives: <IDEAS that we are introducing newly to the field> (3-4 bullet points) <- should contain `keywords`\n",
    "# Architecture: (you can't draw just tell the steps or names of them)\n",
    "# Application Areas: `3-5` areas where our research can have an impact\n",
    "# conclusion: \n",
    "# Project Plan: \n",
    "```\n",
    "\n",
    "\n",
    "constraints: \n",
    "```\n",
    "- It should not be too hard to develop for beginners \n",
    "- Should have a small scope not requiring years of scope\n",
    "- At the end explain what all topics we need to study within 3 weeks to be knoledge-ible enough to make this project. \n",
    "```\n",
    "\n",
    "\n",
    "COME UP WITH ONLY ONE IDEA\n",
    "\"\"\"\n",
    "\n",
    "md_prompt = PromptTemplate.from_template(suggest_ideas_tempalte)\n",
    "\n",
    "synopsis_format = md_prompt.format(\n",
    "    lit_review=lit_review_md\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high tempreature means more creative ideas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopsis = llm.invoke(synopsis_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Project Title:  KG-LLM: Knowledge Graph Enhanced Large Language Models for Improved Reasoning and Question Answering\n",
       "\n",
       "## Paper Title: KGAT: Knowledge Graph Attention Network for Recommendation\n",
       "\n",
       "## Abstract: \n",
       "\n",
       "This project explores the integration of knowledge graphs (KGs) with large language models (LLMs) to enhance their reasoning capabilities and improve performance in question answering tasks. We aim to leverage the structured information in KGs to provide LLMs with a richer context and improved understanding of relationships between entities, leading to more accurate and nuanced answers. This approach builds upon existing research on KGAT (Knowledge Graph Attention Network) for recommendation systems and leverages the power of LLMs for natural language processing tasks.\n",
       "\n",
       "## Motivation:\n",
       "\n",
       "* **Limited Reasoning Capabilities of LLMs:** Current LLMs struggle with complex reasoning tasks, especially when dealing with factual knowledge or understanding intricate relationships between entities.\n",
       "* **Knowledge Graph Potential:** KGs offer a structured representation of factual knowledge, providing LLMs with a valuable source of information to enhance their reasoning abilities.\n",
       "* **Improving Question Answering:** Integrating KGs into LLMs can significantly improve question answering performance by providing a more comprehensive context and enabling the model to infer relationships between entities.\n",
       "* **Real-world Applications:** Enhanced question answering systems with improved reasoning capabilities have applications in various domains such as customer service, information retrieval, and education.\n",
       "\n",
       "## Objectives:\n",
       "\n",
       "* **KG-LLM Architecture:**  Develop a novel architecture that effectively integrates a knowledge graph into the structure of a pre-trained LLM.\n",
       "* **Knowledge-Guided Reasoning:** Implement mechanisms within the KG-LLM that enable the model to leverage KG information for improved reasoning and inference.\n",
       "* **Question Answering Evaluation:** Evaluate the performance of the KG-LLM on benchmark question answering datasets, comparing it to traditional LLMs and other KG-based approaches.\n",
       "* **Explainable Reasoning:**  Explore techniques to provide explanations for the reasoning process of the KG-LLM, enhancing transparency and trust in its predictions.\n",
       "\n",
       "## Architecture:\n",
       "\n",
       "The proposed KG-LLM architecture will consist of:\n",
       "\n",
       "1. **Knowledge Graph Embedding:**  Embed the knowledge graph into a vector space using techniques like TransE or RotatE.\n",
       "2. **LLM Integration:** Integrate the embedded KG into the pre-trained LLM, potentially through attention mechanisms or by fine-tuning the LLM on KG-related tasks.\n",
       "3. **Reasoning Mechanism:** Develop a mechanism within the KG-LLM that enables the model to utilize the embedded KG for inference and reasoning during question answering.\n",
       "\n",
       "## Application Areas:\n",
       "\n",
       "* **Customer Service Chatbots:**  Enhance chatbot responses with factual knowledge and improved reasoning capabilities.\n",
       "* **Information Retrieval Systems:**  Improve search results by leveraging KG information to understand user queries more effectively.\n",
       "* **Educational Tools:**  Develop intelligent tutoring systems that can provide explanations and answer student questions more accurately.\n",
       "* **Medical Diagnosis and Treatment:**  Assist medical professionals by providing insights based on medical knowledge graphs and patient data.\n",
       "\n",
       "## Conclusion:\n",
       "\n",
       "This project aims to address the limitations of traditional LLMs by integrating knowledge graphs to enhance their reasoning capabilities and improve question answering performance. The proposed KG-LLM architecture has the potential to significantly impact various domains by providing more accurate and insightful answers, leading to improved decision-making and better user experiences.\n",
       "\n",
       "## Project Plan:\n",
       "\n",
       "**Phase 1 (Week 1-3):**\n",
       "\n",
       "* **Literature Review:**  Thorough study of existing research on KGAT, LLMs, and knowledge-based reasoning.\n",
       "* **Data Acquisition:**  Select and prepare suitable knowledge graphs and question answering datasets.\n",
       "* **Model Selection:**  Choose an appropriate LLM and KG embedding technique.\n",
       "* **Architecture Design:**  Develop the initial design for the KG-LLM architecture.\n",
       "\n",
       "**Phase 2 (Week 4-6):**\n",
       "\n",
       "* **Model Implementation:**  Implement the KG-LLM architecture using deep learning frameworks.\n",
       "* **Training and Evaluation:**  Train the model on the chosen datasets and evaluate its performance on various question answering tasks.\n",
       "* **Analysis and Optimization:**  Analyze the model's performance and identify areas for improvement.\n",
       "* **Explainability Techniques:**  Explore and implement techniques for explaining the KG-LLM's reasoning process.\n",
       "\n",
       "**Phase 3 (Week 7-9):**\n",
       "\n",
       "* **Final Evaluation and Reporting:**  Conduct a comprehensive evaluation of the KG-LLM's performance and document the findings in a research paper.\n",
       "* **Presentation and Demonstration:**  Prepare a presentation and demonstration of the project for final evaluation.\n",
       "* **Project Documentation:**  Create a detailed report documenting the project's methodology, results, and future directions.\n",
       "\n",
       "## Topics to Study:\n",
       "\n",
       "* **Knowledge Graph Embeddings:** Techniques like TransE, RotatE, ComplEx, etc.\n",
       "* **Large Language Models:**  Architecture, pre-training, fine-tuning, and applications.\n",
       "* **Attention Mechanisms:**  Self-attention, multi-head attention, and their role in LLMs.\n",
       "* **Question Answering Techniques:**  Different approaches to question answering, including retrieval-based, generative, and knowledge-based methods.\n",
       "* **Deep Learning Frameworks:**  TensorFlow, PyTorch, or other suitable frameworks for implementing the KG-LLM.\n",
       "\n",
       "This project provides a challenging and rewarding experience for final year B.E. students, exposing them to cutting-edge research in LLMs and knowledge graph integration. By successfully integrating a knowledge graph into an LLM, students will contribute to the advancement of artificial intelligence and gain valuable experience in a rapidly growing field. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(synopsis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
