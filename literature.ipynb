{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY=os.environ['GOOGLE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aayushchaudhary/Git/college/literature-bot/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Set proper name of the path to the paper's pdf`\n",
    "\n",
    "If loading single dcoument: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('./papers/0.pdf')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Docs\n",
    "\n",
    "`here set the directory where you have the papers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(path='./papers/', glob='**/*.pdf', loader_cls=PyPDFLoader)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is loaded page by page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S2275 Attention-based graph neural networks: a\\xa0survey  \\n1 3\\nInter-layer GATs: This kind of works usually select features beyond neural network \\nlayers with multiple feature spaces, not just local neighborhoods. Across the neural net-\\nwork layer, attention in inter-layer GATs can be regarded as an operation of cross-layer \\nfusion of different feature spaces with feature fusion attention. In this term, attention-\\nbased GNNs dynamically select features from different levels, different channels, dif-\\nferent views, or different time slices. Therefore, we further divide these methods into \\nfive sub-categories (i.e., multi-level attention (Liu et\\xa0 al. 2020; Zhang et\\xa0 al. 2022c), \\nmulti-channel (Bo et\\xa0al. 2021; Luan et\\xa0al. 2021), multi-view (Wang et\\xa0al. 2020b; Yuan \\net\\xa0al. 2021b), Spatio-temporal attention (Sankar et\\xa0al. 2018; Lu et\\xa0al. 2019), and time \\nseries attention (Zhang et\\xa0al. 2021c; Zhao et\\xa0al. 2020)). By considering temporal attrib-\\nutes, Spatio-temporal attention usually uses time, spatial attention, or both in dynamic \\ngraphs, while time-series attention needs to construct dynamic graphs from time-series \\ndata first.\\nGraph Transformers: In the past two years, Transformers (Lin et\\xa0al. 2021) have achieved \\nsuperior performance in many tasks of NLP, CV, and GRL. Graph Transformers gener -\\nalize the Transformer architecture to graph representation learning, capturing long-range \\ndependency (Ying et\\xa0 al. 2021). Different from previous methods with local attention, \\nGraph Transformers learn higher-order graph properties directly via global attention. \\nGraph Transformers have developed rapidly in the field of graph deep learning, especially \\nin the task of graph classification on small and medium-sized graphs. We further divide \\nGraph Transformers into two sub-categories, namely standard Transformers (Ying et\\xa0 al. \\n2021) and GNN Transformers (Nguyen et\\xa0al. 2019). Standard Transformers usually utilize \\nthe self-attention mechanism to all nodes of the input graph, ignoring adjacencies between \\nnodes, while GNN Transformers use the GNN layer to obtain adjacency information.Fig. 6  Classification breakdown of methods for attention-based GNNs'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[12].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split all the docs into Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split pages into 1000 word chunks with buffer/overlap of 200 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size= 1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate model embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722946400.395771   43860 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    }
   ],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=GOOGLE_API_KEY)\n",
    "vector_store = Chroma.from_documents(documents=texts, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup LLM to be used and chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Query to generate review of all papers: \n",
    "\n",
    "`Should contain all the names of your papers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\"\"\"\n",
    "You are a [Research Assitant] bot. You help with creating [Literature Review]. \n",
    "\n",
    "Input :You will be given access to a [research paper]\n",
    "\n",
    "\n",
    "Task: You have to [extract] the following information:\n",
    "Information To BE extracted is present in backticks: \n",
    "\n",
    "Paper Name\n",
    "Focus Area of the paper\n",
    "Date: month/year\n",
    "Methodology: (eg Qualitative, Quantitative, Review, Conceptual, Report)\n",
    "Key Findings: in 10 words what the paper has implemented/achieved\n",
    "Application: real life potential use cases (summerize in 10 words or 1-2 points)\n",
    "Challenges:  Drawbacks of this paper/approach (summerize 1-2 short points)\n",
    "Opportunities: Future scope/possibilities of paper (summerize in 1-2 points)\n",
    "Dataset: (dataset used in the paper if available else write none) \n",
    "\n",
    "Note: if including multiple points, insert newline character or write a unordered list in markdown wrapped as a string\n",
    "\n",
    "Output Format is inside triple backticks: \n",
    "```\n",
    "python\n",
    "[\n",
    "        \n",
    "{\n",
    "\"paper_name\": \"name of the paper 0\",\n",
    ".\n",
    ".\n",
    ".\n",
    "\"opportunities\": \"...\"\n",
    "\"datatset\": \"name of the dataset\"\n",
    "},\n",
    "{\n",
    "\"paper_name\": \"name of the paper 1\",\n",
    ".\n",
    ".\n",
    ".\n",
    "\"opportunities\": \"...\"\n",
    "\"datatset\": \"name of the dataset\"\n",
    "}\n",
    "...\n",
    "]\n",
    "\n",
    "```\n",
    "         \n",
    "Names of the papers: \n",
    "<<<\n",
    "A Survey on Graph Neural Networks for Time\n",
    "Series: Forecasting, Classification, Imputation,\n",
    "and Anomaly Detection, \n",
    "Attention Is All You Need,\n",
    "Exploring Hierarchical Structures for Recommender Systems,\n",
    "Heterogeneous Graph Attention Network, \n",
    "Hyperbolic Graph Attention Network, \n",
    "KGAT: Knowledge Graph Attention Network for Recommendation, \n",
    "Knowledge Graph Embedding Based on Graph Neural Network,\n",
    "Research on the application of Nerual Network model in knowledge graph completion technology,\n",
    "Attention based graph neural networks: a survey, \n",
    "Graph neural networks for visual question answering: a systematic review, \n",
    "A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions\n",
    ">>>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain.invoke({'query': query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result is Stored in `result` object and can be accessed using key: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\n[\\n{\\n\"paper_name\": \"A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection\",\\n\"Focus Area of the paper\": \"Graph Neural Networks for Time Series\",\\n\"Date\": \"12/2022\",\\n\"Methodology\": \"Review\",\\n\"Key Findings\": \"Surveyed GNNs for time series tasks\",\\n\"Application\": \"Forecasting, classification, imputation, anomaly detection\",\\n\"Challenges\": \"Limited data availability, model complexity\",\\n\"Opportunities\": \"Explore new architectures, improve interpretability\",\\n\"datatset\": \"None\"\\n},\\n{\\n\"paper_name\": \"Attention Is All You Need\",\\n\"Focus Area of the paper\": \"Attention Mechanism in Neural Networks\",\\n\"Date\": \"06/2017\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed Transformer architecture based on attention\",\\n\"Application\": \"Machine translation, text summarization\",\\n\"Challenges\": \"High computational cost, lack of interpretability\",\\n\"Opportunities\": \"Explore new attention mechanisms, improve efficiency\",\\n\"datatset\": \"WMT 2014 English-to-German translation task\"\\n},\\n{\\n\"paper_name\": \"Exploring Hierarchical Structures for Recommender Systems\",\\n\"Focus Area of the paper\": \"Hierarchical Structures in Recommender Systems\",\\n\"Date\": \"01/2019\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed hierarchical attention network for recommendation\",\\n\"Application\": \"Recommending products, services, content\",\\n\"Challenges\": \"Data sparsity, cold-start problem\",\\n\"Opportunities\": \"Explore new hierarchical structures, improve personalization\",\\n\"datatset\": \"MovieLens 100K, Amazon Reviews\"\\n},\\n{\\n\"paper_name\": \"Heterogeneous Graph Attention Network\",\\n\"Focus Area of the paper\": \"Heterogeneous Graph Attention Networks\",\\n\"Date\": \"08/2019\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed HGAN for heterogeneous graph learning\",\\n\"Application\": \"Node classification, link prediction\",\\n\"Challenges\": \"Handling different node types and relations\",\\n\"Opportunities\": \"Explore new attention mechanisms for heterogeneous graphs\",\\n\"datatset\": \"Amazon co-purchasing network, DBLP citation network\"\\n},\\n{\\n\"paper_name\": \"Hyperbolic Graph Attention Network\",\\n\"Focus Area of the paper\": \"Hyperbolic Graph Attention Networks\",\\n\"Date\": \"02/2020\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed HGAT for learning on hyperbolic graphs\",\\n\"Application\": \"Node classification, link prediction\",\\n\"Challenges\": \"Handling non-Euclidean space, model complexity\",\\n\"Opportunities\": \"Explore new hyperbolic embedding techniques\",\\n\"datatset\": \"Cora, PubMed, Amazon\"\\n},\\n{\\n\"paper_name\": \"KGAT: Knowledge Graph Attention Network for Recommendation\",\\n\"Focus Area of the paper\": \"Knowledge Graph Attention Networks for Recommendation\",\\n\"Date\": \"06/2019\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed KGAT for incorporating knowledge graphs into recommendation\",\\n\"Application\": \"Recommending products, services, content\",\\n\"Challenges\": \"Scalability, knowledge graph incompleteness\",\\n\"Opportunities\": \"Explore new ways to integrate knowledge graphs\",\\n\"datatset\": \"Amazon, MovieLens\"\\n},\\n{\\n\"paper_name\": \"Knowledge Graph Embedding Based on Graph Neural Network\",\\n\"Focus Area of the paper\": \"Knowledge Graph Embedding using GNNs\",\\n\"Date\": \"03/2020\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed GNN-based embedding for knowledge graphs\",\\n\"Application\": \"Knowledge graph completion, question answering\",\\n\"Challenges\": \"Data sparsity, model complexity\",\\n\"Opportunities\": \"Explore new GNN architectures for knowledge graphs\",\\n\"datatset\": \"FB15k-237, WN18RR\"\\n},\\n{\\n\"paper_name\": \"Research on the application of Nerual Network model in knowledge graph completion technology\",\\n\"Focus Area of the paper\": \"Knowledge Graph Completion using Neural Networks\",\\n\"Date\": \"06/2020\",\\n\"Methodology\": \"Review\",\\n\"Key Findings\": \"Surveyed NN models for knowledge graph completion\",\\n\"Application\": \"Knowledge graph completion, question answering\",\\n\"Challenges\": \"Data sparsity, model complexity\",\\n\"Opportunities\": \"Explore new NN architectures for knowledge graphs\",\\n\"datatset\": \"None\"\\n},\\n{\\n\"paper_name\": \"Attention based graph neural networks: a survey\",\\n\"Focus Area of the paper\": \"Attention Mechanisms in Graph Neural Networks\",\\n\"Date\": \"06/2021\",\\n\"Methodology\": \"Review\",\\n\"Key Findings\": \"Surveyed attention-based GNNs\",\\n\"Application\": \"Node classification, link prediction, graph classification\",\\n\"Challenges\": \"Computational cost, interpretability\",\\n\"Opportunities\": \"Explore new attention mechanisms, improve efficiency\",\\n\"datatset\": \"None\"\\n},\\n{\\n\"paper_name\": \"Graph neural networks for visual question answering: a systematic review\",\\n\"Focus Area of the paper\": \"Graph Neural Networks for Visual Question Answering\",\\n\"Date\": \"08/2021\",\\n\"Methodology\": \"Review\",\\n\"Key Findings\": \"Surveyed GNNs for visual question answering\",\\n\"Application\": \"Visual question answering, image captioning\",\\n\"Challenges\": \"Data scarcity, model complexity\",\\n\"Opportunities\": \"Explore new GNN architectures for visual tasks\",\\n\"datatset\": \"None\"\\n},\\n{\\n\"paper_name\": \"A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions\",\\n\"Focus Area of the paper\": \"Graph Neural Networks\",\\n\"Date\": \"12/2020\",\\n\"Methodology\": \"Review\",\\n\"Key Findings\": \"Comprehensive review of GNNs\",\\n\"Application\": \"Node classification, link prediction, graph classification\",\\n\"Challenges\": \"Scalability, interpretability, data sparsity\",\\n\"Opportunities\": \"Explore new GNN architectures, improve interpretability\",\\n\"datatset\": \"None\"\\n}\\n]\\n```'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Output is formatted in a way that it can be parsed as a python list of dicts/JSON objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "[\n",
       "{\n",
       "\"paper_name\": \"A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection\",\n",
       "\"Focus Area of the paper\": \"Graph Neural Networks for Time Series\",\n",
       "\"Date\": \"12/2022\",\n",
       "\"Methodology\": \"Review\",\n",
       "\"Key Findings\": \"Surveyed GNNs for time series tasks\",\n",
       "\"Application\": \"Forecasting, classification, imputation, anomaly detection\",\n",
       "\"Challenges\": \"Limited data availability, model complexity\",\n",
       "\"Opportunities\": \"Explore new architectures, improve interpretability\",\n",
       "\"datatset\": \"None\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Attention Is All You Need\",\n",
       "\"Focus Area of the paper\": \"Attention Mechanism in Neural Networks\",\n",
       "\"Date\": \"06/2017\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed Transformer architecture based on attention\",\n",
       "\"Application\": \"Machine translation, text summarization\",\n",
       "\"Challenges\": \"High computational cost, lack of interpretability\",\n",
       "\"Opportunities\": \"Explore new attention mechanisms, improve efficiency\",\n",
       "\"datatset\": \"WMT 2014 English-to-German translation task\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Exploring Hierarchical Structures for Recommender Systems\",\n",
       "\"Focus Area of the paper\": \"Hierarchical Structures in Recommender Systems\",\n",
       "\"Date\": \"01/2019\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed hierarchical attention network for recommendation\",\n",
       "\"Application\": \"Recommending products, services, content\",\n",
       "\"Challenges\": \"Data sparsity, cold-start problem\",\n",
       "\"Opportunities\": \"Explore new hierarchical structures, improve personalization\",\n",
       "\"datatset\": \"MovieLens 100K, Amazon Reviews\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Heterogeneous Graph Attention Network\",\n",
       "\"Focus Area of the paper\": \"Heterogeneous Graph Attention Networks\",\n",
       "\"Date\": \"08/2019\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed HGAN for heterogeneous graph learning\",\n",
       "\"Application\": \"Node classification, link prediction\",\n",
       "\"Challenges\": \"Handling different node types and relations\",\n",
       "\"Opportunities\": \"Explore new attention mechanisms for heterogeneous graphs\",\n",
       "\"datatset\": \"Amazon co-purchasing network, DBLP citation network\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Hyperbolic Graph Attention Network\",\n",
       "\"Focus Area of the paper\": \"Hyperbolic Graph Attention Networks\",\n",
       "\"Date\": \"02/2020\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed HGAT for learning on hyperbolic graphs\",\n",
       "\"Application\": \"Node classification, link prediction\",\n",
       "\"Challenges\": \"Handling non-Euclidean space, model complexity\",\n",
       "\"Opportunities\": \"Explore new hyperbolic embedding techniques\",\n",
       "\"datatset\": \"Cora, PubMed, Amazon\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"KGAT: Knowledge Graph Attention Network for Recommendation\",\n",
       "\"Focus Area of the paper\": \"Knowledge Graph Attention Networks for Recommendation\",\n",
       "\"Date\": \"06/2019\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed KGAT for incorporating knowledge graphs into recommendation\",\n",
       "\"Application\": \"Recommending products, services, content\",\n",
       "\"Challenges\": \"Scalability, knowledge graph incompleteness\",\n",
       "\"Opportunities\": \"Explore new ways to integrate knowledge graphs\",\n",
       "\"datatset\": \"Amazon, MovieLens\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Knowledge Graph Embedding Based on Graph Neural Network\",\n",
       "\"Focus Area of the paper\": \"Knowledge Graph Embedding using GNNs\",\n",
       "\"Date\": \"03/2020\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed GNN-based embedding for knowledge graphs\",\n",
       "\"Application\": \"Knowledge graph completion, question answering\",\n",
       "\"Challenges\": \"Data sparsity, model complexity\",\n",
       "\"Opportunities\": \"Explore new GNN architectures for knowledge graphs\",\n",
       "\"datatset\": \"FB15k-237, WN18RR\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Research on the application of Nerual Network model in knowledge graph completion technology\",\n",
       "\"Focus Area of the paper\": \"Knowledge Graph Completion using Neural Networks\",\n",
       "\"Date\": \"06/2020\",\n",
       "\"Methodology\": \"Review\",\n",
       "\"Key Findings\": \"Surveyed NN models for knowledge graph completion\",\n",
       "\"Application\": \"Knowledge graph completion, question answering\",\n",
       "\"Challenges\": \"Data sparsity, model complexity\",\n",
       "\"Opportunities\": \"Explore new NN architectures for knowledge graphs\",\n",
       "\"datatset\": \"None\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Attention based graph neural networks: a survey\",\n",
       "\"Focus Area of the paper\": \"Attention Mechanisms in Graph Neural Networks\",\n",
       "\"Date\": \"06/2021\",\n",
       "\"Methodology\": \"Review\",\n",
       "\"Key Findings\": \"Surveyed attention-based GNNs\",\n",
       "\"Application\": \"Node classification, link prediction, graph classification\",\n",
       "\"Challenges\": \"Computational cost, interpretability\",\n",
       "\"Opportunities\": \"Explore new attention mechanisms, improve efficiency\",\n",
       "\"datatset\": \"None\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Graph neural networks for visual question answering: a systematic review\",\n",
       "\"Focus Area of the paper\": \"Graph Neural Networks for Visual Question Answering\",\n",
       "\"Date\": \"08/2021\",\n",
       "\"Methodology\": \"Review\",\n",
       "\"Key Findings\": \"Surveyed GNNs for visual question answering\",\n",
       "\"Application\": \"Visual question answering, image captioning\",\n",
       "\"Challenges\": \"Data scarcity, model complexity\",\n",
       "\"Opportunities\": \"Explore new GNN architectures for visual tasks\",\n",
       "\"datatset\": \"None\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions\",\n",
       "\"Focus Area of the paper\": \"Graph Neural Networks\",\n",
       "\"Date\": \"12/2020\",\n",
       "\"Methodology\": \"Review\",\n",
       "\"Key Findings\": \"Comprehensive review of GNNs\",\n",
       "\"Application\": \"Node classification, link prediction, graph classification\",\n",
       "\"Challenges\": \"Scalability, interpretability, data sparsity\",\n",
       "\"Opportunities\": \"Explore new GNN architectures, improve interpretability\",\n",
       "\"datatset\": \"None\"\n",
       "}\n",
       "]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Automated Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cleaner approach to run one by one: \n",
    "- will help with limited context\n",
    "- possibly better accuracy and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_promp_template = (\"\"\"\n",
    "You are a [Research Assitant] bot. You help with creating [Literature Review]. \n",
    "\n",
    "Input :You will be given access to a [research paper]\n",
    "\n",
    "\n",
    "Task: You have to [extract] the following information:\n",
    "Information To BE extracted is present in backticks: \n",
    "\n",
    "Paper Name\n",
    "Focus Area of the paper\n",
    "Date: month/year\n",
    "Methodology: (eg Qualitative, Quantitative, Review, Conceptual, Report)\n",
    "Key Findings: in 10 words what the paper has implemented/achieved\n",
    "Application: real life potential use cases (summerize in 10 words or 1-2 points)\n",
    "Challenges:  Drawbacks of this paper/approach (summerize 1-2 short points)\n",
    "Opportunities: Future scope/possibilities of paper (summerize in 1-2 points)\n",
    "Dataset: (dataset used in the paper if available else write none) \n",
    "\n",
    "Note: if including multiple points, insert newline character or write a unordered list in markdown wrapped as a string\n",
    "\n",
    "Output Format is inside triple backticks: \n",
    "```\n",
    "json      \n",
    "{\n",
    "\"paper_name\": \"name of the paper 0\",\n",
    ".\n",
    ".\n",
    ".\n",
    "\"opportunities\": \"...\"\n",
    "\"datatset\": \"name of the dataset\"\n",
    "}\n",
    "```\n",
    "              \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_template = \"\"\"\n",
    "{system_prompt_template}\n",
    "Generate a literature review based on this research paper\n",
    "{paper_name}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "final_prompt_template= PromptTemplate.from_template(user_query_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All our processing for the paper consolidated into one function** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_paper(file_path, paper_name: str): \n",
    "    loader = PyPDFLoader(file_path)\n",
    "    data = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size= 1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(data)\n",
    "    \n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=GOOGLE_API_KEY)\n",
    "    vector_store = Chroma.from_documents(documents=texts, embedding=embeddings)\n",
    "\n",
    "    retriever = vector_store.as_retriever()\n",
    "    \n",
    "    llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\", \n",
    "        retriever=retriever,\n",
    "        return_source_documents=True    \n",
    "    )\n",
    "\n",
    "\n",
    "    final_prompt = final_prompt_template.format(\n",
    "        system_prompt_template=system_prompt, \n",
    "        paper_name =paper_name\n",
    "    )\n",
    "    result = qa_chain.invoke({'query': final_prompt})\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A simple functions to parseout the JSON\n",
    "- Alternatively you can setup JSONOutputParser from langchain.outputparsers (reqires additional setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def extract_and_append_json(json_string):\n",
    "    # Remove the ```json and ``` markers if present\n",
    "    json_string = json_string.strip()\n",
    "    if json_string.startswith(\"```json\"):\n",
    "        json_string = json_string[7:]\n",
    "    if json_string.endswith(\"```\"):\n",
    "        json_string = json_string[:-3]\n",
    "    \n",
    "    # Parse the JSON string\n",
    "    try:\n",
    "        json_data = json.loads(json_string)\n",
    "        \n",
    "        # Append the parsed JSON to the result list\n",
    "        return json_data\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name of all the papers you're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paper_names = [\n",
    "\"Self-Supervised Learning of Graph Neural Networks\",\n",
    "\"\"\"A Survey on Graph Neural Networks for Time\n",
    "Series: Forecasting, Classification, Imputation,\n",
    "and Anomaly Detection\"\"\", \n",
    "\"\"\"Attention Is All You Need\"\"\",\n",
    "\"\"\"Exploring Hierarchical Structures for Recommender Systems\"\"\",\n",
    "\"\"\"Heterogeneous Graph Attention Network\"\"\", \n",
    "\"\"\"Hyperbolic Graph Attention Network\"\"\", \n",
    "\"\"\"KGAT: Knowledge Graph Attention Network for Recommendation\"\"\", \n",
    "\"\"\"Knowledge Graph Embedding Based on Graph Neural Network\"\"\",\n",
    "\"\"\"Research on the application of Nerual Network model in knowledge graph completion technology\"\"\",\n",
    "\"\"\"Attention based graph neural networks: a survey\"\"\", \n",
    "\"\"\"Graph neural networks for visual question answering: a systematic review\"\"\", \n",
    "\"\"\"A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions\"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call Chain on each paper individually and parse output into dicts, stored into `reslist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_directory = './papers'\n",
    "\n",
    "#stores the result\n",
    "reslist = []\n",
    "ct = 0\n",
    "\n",
    "for filename in os.listdir(papers_directory):\n",
    "    # if ct > 1: break\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(papers_directory, filename)\n",
    "        print(f\"Processing: {file_path}\")\n",
    "        num = int(filename[:-4])\n",
    "\n",
    "\n",
    "        result = process_paper(file_path, paper_names[ct])\n",
    "\n",
    "        print(f\"Processed Paper: {paper_names[ct]}\")\n",
    "\n",
    "        reslist.append(extract_and_append_json(result['result']))\n",
    "    ct+=1\n",
    "\n",
    "reslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Accessing One Object```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_name': 'Self-Supervised Learning of Graph Neural Networks',\n",
       " 'focus_area': 'Graph Neural Networks',\n",
       " 'date': '12/2020',\n",
       " 'methodology': 'Review',\n",
       " 'key_findings': 'Survey of self-supervised learning methods for GNNs',\n",
       " 'application': 'Improve GNN performance on tasks with limited labeled data',\n",
       " 'challenges': '- Difficulty in designing effective self-supervision tasks for GNNs\\n-  Limited empirical evaluation of self-supervised GNN methods',\n",
       " 'opportunities': '- Explore new self-supervision tasks for GNNs\\n- Develop more robust and scalable self-supervised GNN methods',\n",
       " 'dataset': 'None'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reslist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- Explore new self-supervision tasks for GNNs\n",
       "- Develop more robust and scalable self-supervised GNN methods"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(reslist[0]['opportunities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also create another chain that takes in these json lists and turn those into MD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not then just return MD instead of JSON in first place? \n",
    "- allows to process each paper individually\n",
    "- can feed into other chains easily\n",
    "- cleaner and structured approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_markdown_tempalte = \"\"\"\n",
    "Given a list of JSON objects inside triple backticks. \\\n",
    "Generate a table in [Markdown] using the keys as Table Columns\n",
    "\n",
    "```{list_of_papers}```\n",
    "\"\"\"\n",
    "\n",
    "md_prompt = PromptTemplate.from_template(create_markdown_tempalte)\n",
    "\n",
    "fin_prompt = md_prompt.format(\n",
    "    list_of_papers=reslist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_review_md = llm.invoke(fin_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| paper_name | focus_area | date | methodology | key_findings | application | challenges | opportunities | dataset |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Self-Supervised Learning of Graph Neural Networks | Graph Neural Networks | 12/2020 | Review | Survey of self-supervised learning methods for GNNs | Improve GNN performance on tasks with limited labeled data | - Difficulty in designing effective self-supervision tasks for GNNs<br>-  Limited empirical evaluation of self-supervised GNN methods | - Explore new self-supervision tasks for GNNs<br>- Develop more robust and scalable self-supervised GNN methods | None |\n",
       "| A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection | Graph Neural Networks for Time Series Analysis | August/2021 | Review | Surveys GNNs for time series analysis tasks | Forecasting, classification, imputation, anomaly detection in various domains | - Limited availability of large-scale time series graph datasets<br>- Difficulty in designing effective graph structures for complex time series data | - Development of novel GNN architectures for specific time series tasks<br>- Exploration of hybrid GNN models that combine different graph architectures | None |\n",
       "| Attention Is All You Need | Natural Language Processing, Machine Translation | 6/2017 | Quantitative | Transformer model, self-attention mechanism, parallel computation | Machine translation, text summarization, question answering | Computational complexity, training data requirements | Improving efficiency, exploring new applications | WMT 2014 English-to-German, WMT 2014 English-to-French |\n",
       "| Exploring Hierarchical Structures for Recommender Systems | Recommender Systems | 07/2022 | Conceptual | Exploring implicit hierarchies for recommender systems | Improved recommendation performance in various domains | - Difficulty in identifying implicit hierarchies<br>- Computational complexity of exploring hierarchies | - Development of efficient algorithms for hierarchy exploration<br>- Integration of hierarchical structures with other recommendation techniques | None |\n",
       "| Heterogeneous Graph Attention Network | Graph Neural Networks | N/A | Quantitative | Proposed a heterogeneous graph attention network | Learning from heterogeneous graphs, real-world data | Scalability, handling large graphs | Further research on heterogeneous graph representation | None |\n",
       "| Hyperbolic Graph Attention Network | Graph Neural Networks (GNNs) | 04/2022 | Quantitative | Hyperbolic space improves GNNs performance | Social network analysis, recommendation systems | Hyperbolic space requires more computation | Explore different hyperbolic space models | Cora, PubMed, Amazon |\n",
       "| KGAT: Knowledge Graph Attention Network for Recommendation | Recommendation Systems | 00/2023 | Quantitative | Knowledge graph attention network for recommendation | Personalized recommendation systems, Improved recommendation accuracy | Computational complexity, Data sparsity | Extension to other domains, Integration with other recommendation techniques | None |\n",
       "| Knowledge Graph Embedding Based on Graph Neural Networks | Knowledge Graph Completion | 12/2019 | Quantitative | Embedding knowledge graphs using graph neural networks | Improving knowledge graph completion accuracy | - Difficulty in handling large graphs<br>- Limited scalability for very large datasets | - Exploring new graph neural network architectures for better performance<br>- Integrating external information into the embedding process | None |\n",
       "| Knowledge Graph Completion Based on Recurrent Neural Network | Knowledge Graph Completion | None | Quantitative | Proposed a recurrent neural network model for knowledge graph completion | Improve knowledge graph completion accuracy and efficiency | Limited by the size and complexity of the knowledge graph | Explore more complex neural network architectures and integrate external knowledge sources | None |\n",
       "| Attention-based Graph Neural Networks: A Survey | Attention mechanisms in Graph Neural Networks (GNNs) | 01/2023 | Review | Categorization and analysis of attention-based GNNs | Node classification, link prediction, graph generation | - Lack of standardized evaluation metrics<br>- Scalability issues for large graphs | - Development of novel attention mechanisms<br>- Exploration of attention-based GNNs for complex tasks | None |\n",
       "| Graph neural networks for visual question answering: a systematic review | Visual Question Answering (VQA) using Graph Neural Networks | November/2023 | Systematic Review | Graph networks enhance VQA performance. | Answering questions about images, Image captioning, Object recognition | Data scarcity and computational complexity | Improving graph network architectures, Exploring new applications | None |\n",
       "| A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions | Graph Neural Networks | 1/2024 | Review | Survey of graph neural networks and their applications | -  Drug discovery<br>-  Social network analysis | -  Scalability of GNNs<br>-  Overfitting to specific graph structures | -  Development of more efficient GNN architectures<br>-  Application of GNNs to new domains | None | \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(lit_review_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
