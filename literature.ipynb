{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY=os.environ['GOOGLE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aayushchaudhary/Git/college/literature-bot/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Set proper name of the path to the paper's pdf`\n",
    "\n",
    "If loading single dcoument: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('./papers/0.pdf')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Docs\n",
    "\n",
    "`here set the directory where you have the papers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(path='./papers/', glob='**/*.pdf', loader_cls=PyPDFLoader)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is loaded page by page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S2275 Attention-based graph neural networks: a\\xa0survey  \\n1 3\\nInter-layer GATs: This kind of works usually select features beyond neural network \\nlayers with multiple feature spaces, not just local neighborhoods. Across the neural net-\\nwork layer, attention in inter-layer GATs can be regarded as an operation of cross-layer \\nfusion of different feature spaces with feature fusion attention. In this term, attention-\\nbased GNNs dynamically select features from different levels, different channels, dif-\\nferent views, or different time slices. Therefore, we further divide these methods into \\nfive sub-categories (i.e., multi-level attention (Liu et\\xa0 al. 2020; Zhang et\\xa0 al. 2022c), \\nmulti-channel (Bo et\\xa0al. 2021; Luan et\\xa0al. 2021), multi-view (Wang et\\xa0al. 2020b; Yuan \\net\\xa0al. 2021b), Spatio-temporal attention (Sankar et\\xa0al. 2018; Lu et\\xa0al. 2019), and time \\nseries attention (Zhang et\\xa0al. 2021c; Zhao et\\xa0al. 2020)). By considering temporal attrib-\\nutes, Spatio-temporal attention usually uses time, spatial attention, or both in dynamic \\ngraphs, while time-series attention needs to construct dynamic graphs from time-series \\ndata first.\\nGraph Transformers: In the past two years, Transformers (Lin et\\xa0al. 2021) have achieved \\nsuperior performance in many tasks of NLP, CV, and GRL. Graph Transformers gener -\\nalize the Transformer architecture to graph representation learning, capturing long-range \\ndependency (Ying et\\xa0 al. 2021). Different from previous methods with local attention, \\nGraph Transformers learn higher-order graph properties directly via global attention. \\nGraph Transformers have developed rapidly in the field of graph deep learning, especially \\nin the task of graph classification on small and medium-sized graphs. We further divide \\nGraph Transformers into two sub-categories, namely standard Transformers (Ying et\\xa0 al. \\n2021) and GNN Transformers (Nguyen et\\xa0al. 2019). Standard Transformers usually utilize \\nthe self-attention mechanism to all nodes of the input graph, ignoring adjacencies between \\nnodes, while GNN Transformers use the GNN layer to obtain adjacency information.Fig. 6  Classification breakdown of methods for attention-based GNNs'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[12].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split all the docs into Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split pages into 1000 word chunks with buffer/overlap of 200 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size= 1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate model embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722946400.395771   43860 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    }
   ],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=GOOGLE_API_KEY)\n",
    "vector_store = Chroma.from_documents(documents=texts, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup LLM to be used and chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Query to generate review of all papers: \n",
    "\n",
    "`Should contain all the names of your papers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\"\"\"\n",
    "You are a [Research Assitant] bot. You help with creating [Literature Review]. \n",
    "\n",
    "Input :You will be given access to a [research paper]\n",
    "\n",
    "\n",
    "Task: You have to [extract] the following information:\n",
    "Information To BE extracted is present in backticks: \n",
    "\n",
    "Paper Name\n",
    "Focus Area of the paper\n",
    "Date: month/year\n",
    "Methodology: (eg Qualitative, Quantitative, Review, Conceptual, Report)\n",
    "Key Findings: in 10 words what the paper has implemented/achieved\n",
    "Application: real life potential use cases (summerize in 10 words or 1-2 points)\n",
    "Challenges:  Drawbacks of this paper/approach (summerize 1-2 short points)\n",
    "Opportunities: Future scope/possibilities of paper (summerize in 1-2 points)\n",
    "Dataset: (dataset used in the paper if available else write none) \n",
    "\n",
    "Note: if including multiple points, insert newline character or write a unordered list in markdown wrapped as a string\n",
    "\n",
    "Output Format is inside triple backticks: \n",
    "```\n",
    "python\n",
    "[\n",
    "        \n",
    "{\n",
    "\"paper_name\": \"name of the paper 0\",\n",
    ".\n",
    ".\n",
    ".\n",
    "\"opportunities\": \"...\"\n",
    "\"datatset\": \"name of the dataset\"\n",
    "},\n",
    "{\n",
    "\"paper_name\": \"name of the paper 1\",\n",
    ".\n",
    ".\n",
    ".\n",
    "\"opportunities\": \"...\"\n",
    "\"datatset\": \"name of the dataset\"\n",
    "}\n",
    "...\n",
    "]\n",
    "\n",
    "```\n",
    "         \n",
    "Names of the papers: \n",
    "<<<\n",
    "A Survey on Graph Neural Networks for Time\n",
    "Series: Forecasting, Classification, Imputation,\n",
    "and Anomaly Detection, \n",
    "Attention Is All You Need,\n",
    "Exploring Hierarchical Structures for Recommender Systems,\n",
    "Heterogeneous Graph Attention Network, \n",
    "Hyperbolic Graph Attention Network, \n",
    "KGAT: Knowledge Graph Attention Network for Recommendation, \n",
    "Knowledge Graph Embedding Based on Graph Neural Network,\n",
    "Research on the application of Nerual Network model in knowledge graph completion technology,\n",
    "Attention based graph neural networks: a survey, \n",
    "Graph neural networks for visual question answering: a systematic review, \n",
    "A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions\n",
    ">>>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain.invoke({'query': query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result is Stored in `result` object and can be accessed using key: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\n[\\n{\\n\"paper_name\": \"A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection\",\\n\"Focus Area of the paper\": \"Graph Neural Networks for Time Series\",\\n\"Date\": \"12/2022\",\\n\"Methodology\": \"Review\",\\n\"Key Findings\": \"Surveyed GNNs for time series tasks\",\\n\"Application\": \"Forecasting, classification, imputation, anomaly detection\",\\n\"Challenges\": \"Limited data availability, model complexity\",\\n\"Opportunities\": \"Explore new architectures, improve interpretability\",\\n\"datatset\": \"None\"\\n},\\n{\\n\"paper_name\": \"Attention Is All You Need\",\\n\"Focus Area of the paper\": \"Attention Mechanism in Neural Networks\",\\n\"Date\": \"06/2017\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed Transformer architecture based on attention\",\\n\"Application\": \"Machine translation, text summarization\",\\n\"Challenges\": \"High computational cost, lack of interpretability\",\\n\"Opportunities\": \"Explore new attention mechanisms, improve efficiency\",\\n\"datatset\": \"WMT 2014 English-to-German translation task\"\\n},\\n{\\n\"paper_name\": \"Exploring Hierarchical Structures for Recommender Systems\",\\n\"Focus Area of the paper\": \"Hierarchical Structures in Recommender Systems\",\\n\"Date\": \"01/2019\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed hierarchical attention network for recommendation\",\\n\"Application\": \"Recommending products, services, content\",\\n\"Challenges\": \"Data sparsity, cold-start problem\",\\n\"Opportunities\": \"Explore new hierarchical structures, improve personalization\",\\n\"datatset\": \"MovieLens 100K, Amazon Reviews\"\\n},\\n{\\n\"paper_name\": \"Heterogeneous Graph Attention Network\",\\n\"Focus Area of the paper\": \"Heterogeneous Graph Attention Networks\",\\n\"Date\": \"08/2019\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed HGAN for heterogeneous graph learning\",\\n\"Application\": \"Node classification, link prediction\",\\n\"Challenges\": \"Handling different node types and relations\",\\n\"Opportunities\": \"Explore new attention mechanisms for heterogeneous graphs\",\\n\"datatset\": \"Amazon co-purchasing network, DBLP citation network\"\\n},\\n{\\n\"paper_name\": \"Hyperbolic Graph Attention Network\",\\n\"Focus Area of the paper\": \"Hyperbolic Graph Attention Networks\",\\n\"Date\": \"02/2020\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed HGAT for learning on hyperbolic graphs\",\\n\"Application\": \"Node classification, link prediction\",\\n\"Challenges\": \"Handling non-Euclidean space, model complexity\",\\n\"Opportunities\": \"Explore new hyperbolic embedding techniques\",\\n\"datatset\": \"Cora, PubMed, Amazon\"\\n},\\n{\\n\"paper_name\": \"KGAT: Knowledge Graph Attention Network for Recommendation\",\\n\"Focus Area of the paper\": \"Knowledge Graph Attention Networks for Recommendation\",\\n\"Date\": \"06/2019\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed KGAT for incorporating knowledge graphs into recommendation\",\\n\"Application\": \"Recommending products, services, content\",\\n\"Challenges\": \"Scalability, knowledge graph incompleteness\",\\n\"Opportunities\": \"Explore new ways to integrate knowledge graphs\",\\n\"datatset\": \"Amazon, MovieLens\"\\n},\\n{\\n\"paper_name\": \"Knowledge Graph Embedding Based on Graph Neural Network\",\\n\"Focus Area of the paper\": \"Knowledge Graph Embedding using GNNs\",\\n\"Date\": \"03/2020\",\\n\"Methodology\": \"Quantitative\",\\n\"Key Findings\": \"Proposed GNN-based embedding for knowledge graphs\",\\n\"Application\": \"Knowledge graph completion, question answering\",\\n\"Challenges\": \"Data sparsity, model complexity\",\\n\"Opportunities\": \"Explore new GNN architectures for knowledge graphs\",\\n\"datatset\": \"FB15k-237, WN18RR\"\\n},\\n{\\n\"paper_name\": \"Research on the application of Nerual Network model in knowledge graph completion technology\",\\n\"Focus Area of the paper\": \"Knowledge Graph Completion using Neural Networks\",\\n\"Date\": \"06/2020\",\\n\"Methodology\": \"Review\",\\n\"Key Findings\": \"Surveyed NN models for knowledge graph completion\",\\n\"Application\": \"Knowledge graph completion, question answering\",\\n\"Challenges\": \"Data sparsity, model complexity\",\\n\"Opportunities\": \"Explore new NN architectures for knowledge graphs\",\\n\"datatset\": \"None\"\\n},\\n{\\n\"paper_name\": \"Attention based graph neural networks: a survey\",\\n\"Focus Area of the paper\": \"Attention Mechanisms in Graph Neural Networks\",\\n\"Date\": \"06/2021\",\\n\"Methodology\": \"Review\",\\n\"Key Findings\": \"Surveyed attention-based GNNs\",\\n\"Application\": \"Node classification, link prediction, graph classification\",\\n\"Challenges\": \"Computational cost, interpretability\",\\n\"Opportunities\": \"Explore new attention mechanisms, improve efficiency\",\\n\"datatset\": \"None\"\\n},\\n{\\n\"paper_name\": \"Graph neural networks for visual question answering: a systematic review\",\\n\"Focus Area of the paper\": \"Graph Neural Networks for Visual Question Answering\",\\n\"Date\": \"08/2021\",\\n\"Methodology\": \"Review\",\\n\"Key Findings\": \"Surveyed GNNs for visual question answering\",\\n\"Application\": \"Visual question answering, image captioning\",\\n\"Challenges\": \"Data scarcity, model complexity\",\\n\"Opportunities\": \"Explore new GNN architectures for visual tasks\",\\n\"datatset\": \"None\"\\n},\\n{\\n\"paper_name\": \"A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions\",\\n\"Focus Area of the paper\": \"Graph Neural Networks\",\\n\"Date\": \"12/2020\",\\n\"Methodology\": \"Review\",\\n\"Key Findings\": \"Comprehensive review of GNNs\",\\n\"Application\": \"Node classification, link prediction, graph classification\",\\n\"Challenges\": \"Scalability, interpretability, data sparsity\",\\n\"Opportunities\": \"Explore new GNN architectures, improve interpretability\",\\n\"datatset\": \"None\"\\n}\\n]\\n```'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Output is formatted in a way that it can be parsed as a python list of dicts/JSON objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "[\n",
       "{\n",
       "\"paper_name\": \"A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection\",\n",
       "\"Focus Area of the paper\": \"Graph Neural Networks for Time Series\",\n",
       "\"Date\": \"12/2022\",\n",
       "\"Methodology\": \"Review\",\n",
       "\"Key Findings\": \"Surveyed GNNs for time series tasks\",\n",
       "\"Application\": \"Forecasting, classification, imputation, anomaly detection\",\n",
       "\"Challenges\": \"Limited data availability, model complexity\",\n",
       "\"Opportunities\": \"Explore new architectures, improve interpretability\",\n",
       "\"datatset\": \"None\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Attention Is All You Need\",\n",
       "\"Focus Area of the paper\": \"Attention Mechanism in Neural Networks\",\n",
       "\"Date\": \"06/2017\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed Transformer architecture based on attention\",\n",
       "\"Application\": \"Machine translation, text summarization\",\n",
       "\"Challenges\": \"High computational cost, lack of interpretability\",\n",
       "\"Opportunities\": \"Explore new attention mechanisms, improve efficiency\",\n",
       "\"datatset\": \"WMT 2014 English-to-German translation task\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Exploring Hierarchical Structures for Recommender Systems\",\n",
       "\"Focus Area of the paper\": \"Hierarchical Structures in Recommender Systems\",\n",
       "\"Date\": \"01/2019\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed hierarchical attention network for recommendation\",\n",
       "\"Application\": \"Recommending products, services, content\",\n",
       "\"Challenges\": \"Data sparsity, cold-start problem\",\n",
       "\"Opportunities\": \"Explore new hierarchical structures, improve personalization\",\n",
       "\"datatset\": \"MovieLens 100K, Amazon Reviews\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Heterogeneous Graph Attention Network\",\n",
       "\"Focus Area of the paper\": \"Heterogeneous Graph Attention Networks\",\n",
       "\"Date\": \"08/2019\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed HGAN for heterogeneous graph learning\",\n",
       "\"Application\": \"Node classification, link prediction\",\n",
       "\"Challenges\": \"Handling different node types and relations\",\n",
       "\"Opportunities\": \"Explore new attention mechanisms for heterogeneous graphs\",\n",
       "\"datatset\": \"Amazon co-purchasing network, DBLP citation network\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Hyperbolic Graph Attention Network\",\n",
       "\"Focus Area of the paper\": \"Hyperbolic Graph Attention Networks\",\n",
       "\"Date\": \"02/2020\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed HGAT for learning on hyperbolic graphs\",\n",
       "\"Application\": \"Node classification, link prediction\",\n",
       "\"Challenges\": \"Handling non-Euclidean space, model complexity\",\n",
       "\"Opportunities\": \"Explore new hyperbolic embedding techniques\",\n",
       "\"datatset\": \"Cora, PubMed, Amazon\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"KGAT: Knowledge Graph Attention Network for Recommendation\",\n",
       "\"Focus Area of the paper\": \"Knowledge Graph Attention Networks for Recommendation\",\n",
       "\"Date\": \"06/2019\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed KGAT for incorporating knowledge graphs into recommendation\",\n",
       "\"Application\": \"Recommending products, services, content\",\n",
       "\"Challenges\": \"Scalability, knowledge graph incompleteness\",\n",
       "\"Opportunities\": \"Explore new ways to integrate knowledge graphs\",\n",
       "\"datatset\": \"Amazon, MovieLens\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Knowledge Graph Embedding Based on Graph Neural Network\",\n",
       "\"Focus Area of the paper\": \"Knowledge Graph Embedding using GNNs\",\n",
       "\"Date\": \"03/2020\",\n",
       "\"Methodology\": \"Quantitative\",\n",
       "\"Key Findings\": \"Proposed GNN-based embedding for knowledge graphs\",\n",
       "\"Application\": \"Knowledge graph completion, question answering\",\n",
       "\"Challenges\": \"Data sparsity, model complexity\",\n",
       "\"Opportunities\": \"Explore new GNN architectures for knowledge graphs\",\n",
       "\"datatset\": \"FB15k-237, WN18RR\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Research on the application of Nerual Network model in knowledge graph completion technology\",\n",
       "\"Focus Area of the paper\": \"Knowledge Graph Completion using Neural Networks\",\n",
       "\"Date\": \"06/2020\",\n",
       "\"Methodology\": \"Review\",\n",
       "\"Key Findings\": \"Surveyed NN models for knowledge graph completion\",\n",
       "\"Application\": \"Knowledge graph completion, question answering\",\n",
       "\"Challenges\": \"Data sparsity, model complexity\",\n",
       "\"Opportunities\": \"Explore new NN architectures for knowledge graphs\",\n",
       "\"datatset\": \"None\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Attention based graph neural networks: a survey\",\n",
       "\"Focus Area of the paper\": \"Attention Mechanisms in Graph Neural Networks\",\n",
       "\"Date\": \"06/2021\",\n",
       "\"Methodology\": \"Review\",\n",
       "\"Key Findings\": \"Surveyed attention-based GNNs\",\n",
       "\"Application\": \"Node classification, link prediction, graph classification\",\n",
       "\"Challenges\": \"Computational cost, interpretability\",\n",
       "\"Opportunities\": \"Explore new attention mechanisms, improve efficiency\",\n",
       "\"datatset\": \"None\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"Graph neural networks for visual question answering: a systematic review\",\n",
       "\"Focus Area of the paper\": \"Graph Neural Networks for Visual Question Answering\",\n",
       "\"Date\": \"08/2021\",\n",
       "\"Methodology\": \"Review\",\n",
       "\"Key Findings\": \"Surveyed GNNs for visual question answering\",\n",
       "\"Application\": \"Visual question answering, image captioning\",\n",
       "\"Challenges\": \"Data scarcity, model complexity\",\n",
       "\"Opportunities\": \"Explore new GNN architectures for visual tasks\",\n",
       "\"datatset\": \"None\"\n",
       "},\n",
       "{\n",
       "\"paper_name\": \"A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions\",\n",
       "\"Focus Area of the paper\": \"Graph Neural Networks\",\n",
       "\"Date\": \"12/2020\",\n",
       "\"Methodology\": \"Review\",\n",
       "\"Key Findings\": \"Comprehensive review of GNNs\",\n",
       "\"Application\": \"Node classification, link prediction, graph classification\",\n",
       "\"Challenges\": \"Scalability, interpretability, data sparsity\",\n",
       "\"Opportunities\": \"Explore new GNN architectures, improve interpretability\",\n",
       "\"datatset\": \"None\"\n",
       "}\n",
       "]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Automated Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cleaner approach to run one by one: \n",
    "- will help with limited context\n",
    "- possibly better accuracy and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_promp_template = (\"\"\"\n",
    "You are a [Research Assitant] bot. You help with creating [Literature Review]. \n",
    "\n",
    "Input :You will be given access to a [research paper]\n",
    "\n",
    "\n",
    "Task: You have to [extract] the following information:\n",
    "Information To BE extracted is present in backticks: \n",
    "\n",
    "Paper Name\n",
    "Focus Area of the paper\n",
    "Date: month/year\n",
    "Methodology: (eg Qualitative, Quantitative, Review, Conceptual, Report)\n",
    "Key Findings: in 10 words what the paper has implemented/achieved\n",
    "Application: real life potential use cases (summerize in 10 words or 1-2 points)\n",
    "Challenges:  Drawbacks of this paper/approach (summerize 1-2 short points)\n",
    "Opportunities: Future scope/possibilities of paper (summerize in 1-2 points)\n",
    "Dataset: (dataset used in the paper if available else write none) \n",
    "\n",
    "Note: if including multiple points, insert newline character or write a unordered list in markdown wrapped as a string\n",
    "\n",
    "Output Format is inside triple backticks: \n",
    "```\n",
    "json      \n",
    "{\n",
    "\"paper_name\": \"name of the paper 0\",\n",
    ".\n",
    ".\n",
    ".\n",
    "\"opportunities\": \"...\"\n",
    "\"datatset\": \"name of the dataset\"\n",
    "}\n",
    "```\n",
    "              \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_template = \"\"\"\n",
    "{system_prompt_template}\n",
    "Generate a literature review based on this research paper\n",
    "{paper_name}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "final_prompt_template= PromptTemplate.from_template(user_query_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All our processing for the paper consolidated into one function** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_paper(file_path, paper_name: str): \n",
    "    loader = PyPDFLoader(file_path)\n",
    "    data = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size= 1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(data)\n",
    "    \n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=GOOGLE_API_KEY)\n",
    "    vector_store = Chroma.from_documents(documents=texts, embedding=embeddings)\n",
    "\n",
    "    retriever = vector_store.as_retriever()\n",
    "    \n",
    "    llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\", \n",
    "        retriever=retriever,\n",
    "        return_source_documents=True    \n",
    "    )\n",
    "\n",
    "\n",
    "    final_prompt = final_prompt_template.format(\n",
    "        system_prompt_template=system_promp_template, \n",
    "        paper_name =paper_name\n",
    "    )\n",
    "    result = qa_chain.invoke({'query': final_prompt})\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A simple functions to parseout the JSON\n",
    "- Alternatively you can setup JSONOutputParser from langchain.outputparsers (reqires additional setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def extract_and_append_json(json_string):\n",
    "    # Remove the ```json and ``` markers if present\n",
    "    json_string = json_string.strip()\n",
    "    if json_string.startswith(\"```json\"):\n",
    "        json_string = json_string[7:]\n",
    "    if json_string.endswith(\"```\"):\n",
    "        json_string = json_string[:-3]\n",
    "    \n",
    "    # Parse the JSON string\n",
    "    try:\n",
    "        json_data = json.loads(json_string)\n",
    "        \n",
    "        # Append the parsed JSON to the result list\n",
    "        return json_data\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name of all the papers you're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paper_names = [\n",
    "\"Self-Supervised Learning of Graph Neural Networks\",\n",
    "\"\"\"A Survey on Graph Neural Networks for Time\n",
    "Series: Forecasting, Classification, Imputation,\n",
    "and Anomaly Detection\"\"\", \n",
    "\"\"\"Attention Is All You Need\"\"\",\n",
    "\"\"\"Exploring Hierarchical Structures for Recommender Systems\"\"\",\n",
    "\"\"\"Heterogeneous Graph Attention Network\"\"\", \n",
    "\"\"\"Hyperbolic Graph Attention Network\"\"\", \n",
    "\"\"\"KGAT: Knowledge Graph Attention Network for Recommendation\"\"\", \n",
    "\"\"\"Knowledge Graph Embedding Based on Graph Neural Network\"\"\",\n",
    "\"\"\"Research on the application of Nerual Network model in knowledge graph completion technology\"\"\",\n",
    "\"\"\"Attention based graph neural networks: a survey\"\"\", \n",
    "\"\"\"Graph neural networks for visual question answering: a systematic review\"\"\", \n",
    "\"\"\"A review of graph neural networks: concepts, architectures, techniques, challenges, datasets, applications, and future directions\"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call Chain on each paper individually and parse output into dicts, stored into `reslist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./papers/9.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723138646.874906  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1723138656.991248  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Paper: Self-Supervised Learning of Graph Neural Networks\n",
      "Processing: ./papers/8.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723138660.854935  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1723138665.050557  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Paper: A Survey on Graph Neural Networks for Time\n",
      "Series: Forecasting, Classification, Imputation,\n",
      "and Anomaly Detection\n",
      "Processing: ./papers/10.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723138668.615345  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1723138679.452414  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Paper: Attention Is All You Need\n",
      "Processing: ./papers/11.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723138682.781523  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1723138690.170401  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Paper: Exploring Hierarchical Structures for Recommender Systems\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Processing: ./papers/6.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723138693.339833  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1723138698.169219  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Paper: Heterogeneous Graph Attention Network\n",
      "Processing: ./papers/7.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723138701.065458  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1723138705.217981  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Paper: Hyperbolic Graph Attention Network\n",
      "Processing: ./papers/5.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723138709.676212  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1723138714.782048  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Paper: KGAT: Knowledge Graph Attention Network for Recommendation\n",
      "Processing: ./papers/4.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723138723.948642  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1723138729.487008  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Paper: Knowledge Graph Embedding Based on Graph Neural Network\n",
      "Processing: ./papers/0.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723138734.203401  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1723138743.754238  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Paper: Research on the application of Nerual Network model in knowledge graph completion technology\n",
      "Processing: ./papers/1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723138748.337893  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1723138760.845032  475288 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Paper: Attention based graph neural networks: a survey\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'paper_name': 'Self-Supervised Learning of Graph Neural Networks',\n",
       "  'focus_area_of_the_paper': 'Graph Neural Networks (GNNs)',\n",
       "  'date': '01/2024',\n",
       "  'methodology': 'Review',\n",
       "  'key_findings': 'Review of self-supervised learning for GNNs',\n",
       "  'application': 'Graph classification, node classification, link prediction',\n",
       "  'challenges': 'Lack of large-scale labeled graph data, difficulty in designing effective self-supervised tasks',\n",
       "  'opportunities': 'Development of new self-supervised learning methods for GNNs, exploration of applications in various domains',\n",
       "  'dataset': 'None'},\n",
       " {'paper_name': 'A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection',\n",
       "  'focus_area': 'Graph Neural Networks for Time Series Analysis',\n",
       "  'date': 'August/2021',\n",
       "  'methodology': 'Review',\n",
       "  'key_findings': 'Taxonomy of GNNs for time series tasks',\n",
       "  'application': 'Forecasting, Classification, Imputation, Anomaly Detection',\n",
       "  'challenges': 'Limited research on multi-step forecasting, need for standardized benchmarks',\n",
       "  'opportunities': 'Develop novel GNN architectures for specific time series tasks, explore applications in diverse domains',\n",
       "  'dataset': 'None'},\n",
       " {'paper_name': 'Attention Is All You Need',\n",
       "  'focus_area_of_the_paper': 'Natural Language Processing (NLP), Machine Translation',\n",
       "  'date': '6/2017',\n",
       "  'methodology': 'Quantitative, Conceptual',\n",
       "  'key_findings': 'Proposes Transformer architecture for NLP tasks',\n",
       "  'application': 'Machine translation, text summarization, question answering',\n",
       "  'challenges': 'Computational complexity, lack of interpretability',\n",
       "  'opportunities': 'Further research on Transformer variants, applications in other domains',\n",
       "  'dataset': 'WMT 2014 English-to-German and English-to-French translation tasks'},\n",
       " None,\n",
       " {'paper_name': 'Heterogeneous Graph Attention Network',\n",
       "  'focus_area_of_the_paper': 'Graph Neural Networks',\n",
       "  'date': 'None',\n",
       "  'methodology': 'Quantitative',\n",
       "  'key_findings': 'Proposes a heterogeneous graph attention network',\n",
       "  'application': 'Knowledge graph completion, recommendation systems',\n",
       "  'challenges': '- Difficulty in handling heterogeneous graphs\\n- Scalability issues',\n",
       "  'opportunities': '- Explore new applications of heterogeneous graph networks\\n- Improve scalability and efficiency',\n",
       "  'dataset': 'None'},\n",
       " {'paper_name': 'Hyperbolic Graph Attention Network',\n",
       "  'focus_area_of_the_paper': 'Graph Neural Networks',\n",
       "  'date': '01/2022',\n",
       "  'methodology': 'Quantitative',\n",
       "  'key_findings': 'Developed a novel hyperbolic graph attention network',\n",
       "  'application': '-  Node classification in complex networks\\n-  Recommendation systems',\n",
       "  'challenges': '-  Computational complexity of hyperbolic geometry\\n-  Hyperbolic embedding of large graphs',\n",
       "  'opportunities': '-  Applications in other graph-related tasks\\n-  Exploration of different hyperbolic geometries',\n",
       "  'dataset': 'None'},\n",
       " {'paper_name': 'KGAT: Knowledge Graph Attention Network for Recommendation',\n",
       "  'focus_area_of_the_paper': 'Recommender Systems, Knowledge Graph Embedding',\n",
       "  'date': 'August/2019',\n",
       "  'methodology': 'Quantitative',\n",
       "  'key_findings': 'Attention-based knowledge graph embedding for recommendation',\n",
       "  'application': 'Personalized recommendation, Explainable AI',\n",
       "  'challenges': 'Computational complexity, Scalability for large knowledge graphs',\n",
       "  'opportunities': 'Integration with other recommendation techniques, Exploration of different attention mechanisms',\n",
       "  'dataset': 'Amazon-book, Last-FM, Yelp2018'},\n",
       " {'paper_name': 'Knowledge Graph Embedding Based on Graph Neural Network',\n",
       "  'focus_area': 'Knowledge Graph Embedding',\n",
       "  'date': 'N/A',\n",
       "  'methodology': 'Review',\n",
       "  'key_findings': 'Survey of GNNs for knowledge graph embedding',\n",
       "  'application': 'Improve knowledge graph completion and reasoning',\n",
       "  'challenges': 'Scalability of GNNs for large graphs\\nData sparsity in knowledge graphs',\n",
       "  'opportunities': 'Explore new GNN architectures\\nDevelop efficient training methods',\n",
       "  'dataset': 'None'},\n",
       " {'paper_name': 'Knowledge Graph Completion Based on Recurrent Neural Network',\n",
       "  'focus_area': 'Knowledge Graph Completion',\n",
       "  'date': '01/2023',\n",
       "  'methodology': 'Quantitative',\n",
       "  'key_findings': 'Recurrent neural network for knowledge graph completion',\n",
       "  'application': 'Predicting missing links in knowledge graphs',\n",
       "  'challenges': '-  Limited scalability to large knowledge graphs\\n-  Difficulty in handling complex relationships',\n",
       "  'opportunities': '-  Improving performance with advanced neural network architectures\\n-  Exploring applications in various domains',\n",
       "  'dataset': 'None'},\n",
       " {'paper_name': 'Attention-based graph neural networks: a survey',\n",
       "  'focus_area_of_the_paper': 'Attention-based graph neural networks',\n",
       "  'date': '01/2023',\n",
       "  'methodology': 'Review',\n",
       "  'key_findings': 'Classifies attention-based GNNs into three stages',\n",
       "  'application': 'Various applications in machine learning, such as natural language processing, computer vision, and recommender systems',\n",
       "  'challenges': '-  Limited theoretical understanding of attention-based GNNs\\n-  Computational complexity of attention mechanisms',\n",
       "  'opportunities': '-  Development of more efficient and effective attention mechanisms\\n-  Exploration of new applications for attention-based GNNs',\n",
       "  'dataset': 'None'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_directory = './papers'\n",
    "\n",
    "#stores the result\n",
    "reslist = []\n",
    "ct = 0\n",
    "\n",
    "for filename in os.listdir(papers_directory):\n",
    "    if ct > 9: break\n",
    "    if ct == 2: pass\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(papers_directory, filename)\n",
    "        print(f\"Processing: {file_path}\")\n",
    "        num = int(filename[:-4])\n",
    "\n",
    "\n",
    "        result = process_paper(file_path, paper_names[ct])\n",
    "\n",
    "        print(f\"Processed Paper: {paper_names[ct]}\")\n",
    "\n",
    "        reslist.append(extract_and_append_json(result['result']))\n",
    "    ct+=1\n",
    "\n",
    "reslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Accessing One Object```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_name': 'Knowledge Graph Completion Based on Recurrent Neural Network',\n",
       " 'focus_area': 'Knowledge Graph Completion',\n",
       " 'date': '01/2023',\n",
       " 'methodology': 'Quantitative',\n",
       " 'key_findings': 'Recurrent neural network for knowledge graph completion',\n",
       " 'application': 'Predicting missing links in knowledge graphs',\n",
       " 'challenges': '-  Limited scalability to large knowledge graphs\\n-  Difficulty in handling complex relationships',\n",
       " 'opportunities': '-  Improving performance with advanced neural network architectures\\n-  Exploring applications in various domains',\n",
       " 'dataset': 'None'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reslist[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Develop novel GNN architectures for specific time series tasks, explore applications in diverse domains"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(reslist[1]['opportunities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also create another chain that takes in these json lists and turn those into MD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not then just return MD instead of JSON in first place? \n",
    "- allows to process each paper individually\n",
    "- can feed into other chains easily\n",
    "- cleaner and structured approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_markdown_tempalte = \"\"\"\n",
    "Given a list of JSON objects inside triple backticks. \\\n",
    "Generate a table in [Markdown] using the keys as Table Columns\n",
    "\n",
    "```{list_of_papers}```\n",
    "\"\"\"\n",
    "\n",
    "md_prompt = PromptTemplate.from_template(create_markdown_tempalte)\n",
    "\n",
    "fin_prompt = md_prompt.format(\n",
    "    list_of_papers=reslist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_review_md = llm.invoke(fin_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| paper_name | focus_area_of_the_paper | date | methodology | key_findings | application | challenges | opportunities | dataset |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Self-Supervised Learning of Graph Neural Networks | Graph Neural Networks (GNNs) | 01/2024 | Review | Review of self-supervised learning for GNNs | Graph classification, node classification, link prediction | Lack of large-scale labeled graph data, difficulty in designing effective self-supervised tasks | Development of new self-supervised learning methods for GNNs, exploration of applications in various domains | None |\n",
       "| A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection | Graph Neural Networks for Time Series Analysis | August/2021 | Review | Taxonomy of GNNs for time series tasks | Forecasting, Classification, Imputation, Anomaly Detection | Limited research on multi-step forecasting, need for standardized benchmarks | Develop novel GNN architectures for specific time series tasks, explore applications in diverse domains | None |\n",
       "| Attention Is All You Need | Natural Language Processing (NLP), Machine Translation | 6/2017 | Quantitative, Conceptual | Proposes Transformer architecture for NLP tasks | Machine translation, text summarization, question answering | Computational complexity, lack of interpretability | Further research on Transformer variants, applications in other domains | WMT 2014 English-to-German and English-to-French translation tasks |\n",
       "| Heterogeneous Graph Attention Network | Graph Neural Networks | None | Quantitative | Proposes a heterogeneous graph attention network | Knowledge graph completion, recommendation systems | - Difficulty in handling heterogeneous graphs<br>- Scalability issues | - Explore new applications of heterogeneous graph networks<br>- Improve scalability and efficiency | None |\n",
       "| Hyperbolic Graph Attention Network | Graph Neural Networks | 01/2022 | Quantitative | Developed a novel hyperbolic graph attention network | -  Node classification in complex networks<br>-  Recommendation systems | -  Computational complexity of hyperbolic geometry<br>-  Hyperbolic embedding of large graphs | -  Applications in other graph-related tasks<br>-  Exploration of different hyperbolic geometries | None |\n",
       "| KGAT: Knowledge Graph Attention Network for Recommendation | Recommender Systems, Knowledge Graph Embedding | August/2019 | Quantitative | Attention-based knowledge graph embedding for recommendation | Personalized recommendation, Explainable AI | Computational complexity, Scalability for large knowledge graphs | Integration with other recommendation techniques, Exploration of different attention mechanisms | Amazon-book, Last-FM, Yelp2018 |\n",
       "| Knowledge Graph Embedding Based on Graph Neural Network | Knowledge Graph Embedding | N/A | Review | Survey of GNNs for knowledge graph embedding | Improve knowledge graph completion and reasoning | Scalability of GNNs for large graphs<br>Data sparsity in knowledge graphs | Explore new GNN architectures<br>Develop efficient training methods | None |\n",
       "| Knowledge Graph Completion Based on Recurrent Neural Network | Knowledge Graph Completion | 01/2023 | Quantitative | Recurrent neural network for knowledge graph completion | Predicting missing links in knowledge graphs | -  Limited scalability to large knowledge graphs<br>-  Difficulty in handling complex relationships | -  Improving performance with advanced neural network architectures<br>-  Exploring applications in various domains | None |\n",
       "| Attention-based graph neural networks: a survey | Attention-based graph neural networks | 01/2023 | Review | Classifies attention-based GNNs into three stages | Various applications in machine learning, such as natural language processing, computer vision, and recommender systems | -  Limited theoretical understanding of attention-based GNNs<br>-  Computational complexity of attention mechanisms | -  Development of more efficient and effective attention mechanisms<br>-  Exploration of new applications for attention-based GNNs | None | \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(lit_review_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| paper_name | focus_area_of_the_paper | date | methodology | key_findings | application | challenges | opportunities | dataset |\n",
      "|---|---|---|---|---|---|---|---|---|\n",
      "| Self-Supervised Learning of Graph Neural Networks | Graph Neural Networks (GNNs) | 01/2024 | Review | Review of self-supervised learning for GNNs | Graph classification, node classification, link prediction | Lack of large-scale labeled graph data, difficulty in designing effective self-supervised tasks | Development of new self-supervised learning methods for GNNs, exploration of applications in various domains | None |\n",
      "| A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection | Graph Neural Networks for Time Series Analysis | August/2021 | Review | Taxonomy of GNNs for time series tasks | Forecasting, Classification, Imputation, Anomaly Detection | Limited research on multi-step forecasting, need for standardized benchmarks | Develop novel GNN architectures for specific time series tasks, explore applications in diverse domains | None |\n",
      "| Attention Is All You Need | Natural Language Processing (NLP), Machine Translation | 6/2017 | Quantitative, Conceptual | Proposes Transformer architecture for NLP tasks | Machine translation, text summarization, question answering | Computational complexity, lack of interpretability | Further research on Transformer variants, applications in other domains | WMT 2014 English-to-German and English-to-French translation tasks |\n",
      "| Heterogeneous Graph Attention Network | Graph Neural Networks | None | Quantitative | Proposes a heterogeneous graph attention network | Knowledge graph completion, recommendation systems | - Difficulty in handling heterogeneous graphs<br>- Scalability issues | - Explore new applications of heterogeneous graph networks<br>- Improve scalability and efficiency | None |\n",
      "| Hyperbolic Graph Attention Network | Graph Neural Networks | 01/2022 | Quantitative | Developed a novel hyperbolic graph attention network | -  Node classification in complex networks<br>-  Recommendation systems | -  Computational complexity of hyperbolic geometry<br>-  Hyperbolic embedding of large graphs | -  Applications in other graph-related tasks<br>-  Exploration of different hyperbolic geometries | None |\n",
      "| KGAT: Knowledge Graph Attention Network for Recommendation | Recommender Systems, Knowledge Graph Embedding | August/2019 | Quantitative | Attention-based knowledge graph embedding for recommendation | Personalized recommendation, Explainable AI | Computational complexity, Scalability for large knowledge graphs | Integration with other recommendation techniques, Exploration of different attention mechanisms | Amazon-book, Last-FM, Yelp2018 |\n",
      "| Knowledge Graph Embedding Based on Graph Neural Network | Knowledge Graph Embedding | N/A | Review | Survey of GNNs for knowledge graph embedding | Improve knowledge graph completion and reasoning | Scalability of GNNs for large graphs<br>Data sparsity in knowledge graphs | Explore new GNN architectures<br>Develop efficient training methods | None |\n",
      "| Knowledge Graph Completion Based on Recurrent Neural Network | Knowledge Graph Completion | 01/2023 | Quantitative | Recurrent neural network for knowledge graph completion | Predicting missing links in knowledge graphs | -  Limited scalability to large knowledge graphs<br>-  Difficulty in handling complex relationships | -  Improving performance with advanced neural network architectures<br>-  Exploring applications in various domains | None |\n",
      "| Attention-based graph neural networks: a survey | Attention-based graph neural networks | 01/2023 | Review | Classifies attention-based GNNs into three stages | Various applications in machine learning, such as natural language processing, computer vision, and recommender systems | -  Limited theoretical understanding of attention-based GNNs<br>-  Computational complexity of attention mechanisms | -  Development of more efficient and effective attention mechanisms<br>-  Exploration of new applications for attention-based GNNs | None | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lit_review_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDEA GENERATION - generate ideas for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_ideas_tempalte = \"\"\"\n",
    "You are a `Machine Learning` professor and we your students need to [work on final year project]. \n",
    "For that we have to come up with a list of papers to refer to. \n",
    "\n",
    "`Based on them we have to come up with some research gaps we can fulfil in certain domain or topic` with the goal of: \n",
    "`Write a Research Paper`\n",
    "and `Create the final project`. \n",
    "\n",
    "We need your help to come up with <potential areas> our team can address as final year B.E. students. \n",
    "\n",
    "The project has to build upon existing research and its helpful if we create something that is `trending` in research like LLMs and VLM etc. \n",
    "\n",
    "Our areas of Interest: \n",
    "- Knowledge Representation (very important)\n",
    "- Graph Neural Networks \n",
    "- GAT\n",
    "- Deep Learning \n",
    "- ML \n",
    "- LLMs\n",
    "- NLP\n",
    "- Multimodal AI\n",
    "\n",
    "`our idea has to be novel`\n",
    "\n",
    "\n",
    "END GOAL: \n",
    "<\n",
    "- Come up with 3-5`objectives` that we can have as outcomes of our project\n",
    "- final projects that we can do (END TO END Ideas)\n",
    "- workflow or project plan for that project \n",
    "- scope of project should be 3 months\n",
    "- expose us to a wide range of experiences\n",
    "- help come up with names for the project \n",
    ">\n",
    "\n",
    "\n",
    "Given a list of papers as `Literature Review` in Markdown format.\n",
    "\n",
    "```{lit_review}```\n",
    "\n",
    "\n",
    "Output Format: \n",
    "```markdown\n",
    "# Project Title: Title of Project\n",
    "# Paper Title: Title of Paper\n",
    "# Abstract: 5-10 sentences\n",
    "# Motivation: Why do we want to build on this research (4 bullet points)\n",
    "# Objectives: <IDEAS that we are introducing newly to the field> (3-4 bullet points) <- should contain `keywords`\n",
    "# Architecture: (you can't draw just tell the steps or names of them)\n",
    "# Application Areas: `3-5` areas where our research can have an impact\n",
    "# conclusion: \n",
    "# Project Plan: \n",
    "```\n",
    "\n",
    "\n",
    "constraints: \n",
    "```\n",
    "- It should not be too hard to develop for beginners \n",
    "- Should have a small scope not requiring years of scope\n",
    "- At the end explain what all topics we need to study within 3 weeks to be knoledge-ible enough to make this project. \n",
    "```\n",
    "\n",
    "\n",
    "COME UP WITH ONLY ONE IDEA\n",
    "\"\"\"\n",
    "\n",
    "md_prompt = PromptTemplate.from_template(suggest_ideas_tempalte)\n",
    "\n",
    "synopsis_format = md_prompt.format(\n",
    "    lit_review=lit_review_md\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopsis = llm.invoke(synopsis_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Project Title:  Multimodal Knowledge Graph Completion with Self-Supervised Learning for Enhanced Reasoning\n",
       "\n",
       "## Paper Title: Self-Supervised Learning of Graph Neural Networks\n",
       "\n",
       "## Abstract: \n",
       "\n",
       "This project aims to leverage self-supervised learning techniques to enhance knowledge graph completion, specifically focusing on multimodal knowledge graphs that integrate textual and visual information. By extending existing self-supervised learning methods for graph neural networks (GNNs) to handle multimodal data, we propose a novel approach to improve the accuracy and reasoning capabilities of knowledge graph completion systems. \n",
       "\n",
       "## Motivation:\n",
       "\n",
       "* **Limited Labelled Data:** Knowledge graph completion tasks often struggle with limited labelled data, hindering the performance of traditional supervised learning methods.\n",
       "* **Multimodal Information:**  Real-world knowledge graphs are increasingly multimodal, incorporating textual descriptions, images, and videos. Leveraging this rich information can significantly enhance knowledge representation and reasoning.\n",
       "* **Self-Supervised Learning:** Self-supervised learning offers a promising approach to address the data scarcity problem by learning meaningful representations from unlabeled data.\n",
       "* **GNNs for Knowledge Graphs:** GNNs have proven effective in capturing complex relationships within knowledge graphs, but their performance can be further improved by incorporating multimodal information and leveraging self-supervised learning.\n",
       "\n",
       "## Objectives:\n",
       "\n",
       "* **Multimodal Knowledge Graph Embedding:** Develop a novel GNN architecture for embedding multimodal knowledge graphs, effectively integrating textual and visual information.\n",
       "* **Self-Supervised Learning for Multimodal GNNs:** Design and implement self-supervised learning tasks specifically tailored for multimodal GNNs to learn robust representations from unlabeled data.\n",
       "* **Enhanced Reasoning Capabilities:** Evaluate the proposed approach on various knowledge graph completion tasks, demonstrating improved accuracy and reasoning capabilities compared to existing methods.\n",
       "* **Explainable Knowledge Graph Completion:** Explore methods to provide interpretable explanations for the predicted knowledge graph completions, enhancing the transparency and trust in the system.\n",
       "\n",
       "## Architecture:\n",
       "\n",
       "The proposed architecture will consist of:\n",
       "\n",
       "* **Multimodal Encoder:**  A GNN-based encoder that processes both textual and visual features from the knowledge graph entities.\n",
       "* **Self-Supervised Learning Module:** A module that generates self-supervised learning tasks based on the multimodal features, such as masked entity prediction or multimodal contrastive learning.\n",
       "* **Knowledge Graph Completion Module:**  A module that leverages the learned multimodal embeddings to predict missing links in the knowledge graph.\n",
       "\n",
       "## Application Areas:\n",
       "\n",
       "* **Recommendation Systems:** Enhance recommendation systems by leveraging multimodal knowledge about user preferences and product information.\n",
       "* **Question Answering:**  Improve question answering systems by integrating knowledge from multimodal knowledge graphs, allowing for more comprehensive and informative responses.\n",
       "* **Drug Discovery:** Facilitate drug discovery by identifying potential drug candidates through knowledge graph completion and reasoning over multimodal data about drug-target interactions.\n",
       "* **Social Media Analysis:** Analyze social media data by constructing multimodal knowledge graphs and leveraging them to understand user behavior, sentiment, and trends.\n",
       "\n",
       "## Conclusion: \n",
       "\n",
       "This project aims to significantly advance the field of knowledge graph completion by integrating multimodal information and leveraging self-supervised learning techniques. The proposed approach holds the potential to improve the accuracy and reasoning capabilities of knowledge graph completion systems, enabling impactful applications across various domains.\n",
       "\n",
       "## Project Plan:\n",
       "\n",
       "**Phase 1 (Weeks 1-2):**\n",
       "\n",
       "* **Literature Review:**  Thorough review of existing research on knowledge graph completion, multimodal knowledge graphs, self-supervised learning for GNNs, and explainable AI.\n",
       "* **Data Collection:** Identify and collect suitable multimodal knowledge graph datasets for the project.\n",
       "* **Model Selection:** Choose a suitable GNN architecture for multimodal knowledge graph embedding.\n",
       "* **Self-Supervised Task Design:** Develop specific self-supervised learning tasks for the chosen GNN architecture.\n",
       "\n",
       "**Phase 2 (Weeks 3-4):**\n",
       "\n",
       "* **Model Implementation:** Implement the proposed GNN architecture and self-supervised learning module.\n",
       "* **Training and Evaluation:** Train the model on the chosen dataset and evaluate its performance on various knowledge graph completion tasks.\n",
       "* **Analysis and Visualization:** Analyze the results and visualize the learned multimodal embeddings to gain insights into the model's performance.\n",
       "\n",
       "**Phase 3 (Weeks 5-6):**\n",
       "\n",
       "* **Explainability Exploration:** Explore techniques to provide interpretable explanations for the predicted knowledge graph completions.\n",
       "* **Application Development:** Develop a proof-of-concept application demonstrating the potential impact of the proposed approach in a specific domain (e.g., recommendation systems, question answering).\n",
       "* **Report Writing:** Prepare a detailed project report documenting the research process, findings, and contributions.\n",
       "\n",
       "**Phase 4 (Week 7):**\n",
       "\n",
       "* **Presentation and Defense:** Present the project findings and defend the research to a panel of experts.\n",
       "\n",
       "## Topics to Study:\n",
       "\n",
       "* **Knowledge Representation and Reasoning:** Learn about different knowledge representation schemes, knowledge graph structures, and reasoning techniques.\n",
       "* **Graph Neural Networks:** Gain a thorough understanding of GNN architectures, their applications, and how they learn from graph data.\n",
       "* **Self-Supervised Learning:**  Study different self-supervised learning approaches and their applications in machine learning.\n",
       "* **Multimodal Data Processing:**  Learn about techniques for handling and processing multimodal data, including textual and visual information.\n",
       "* **Explainable AI:** Understand the importance of explainability in machine learning models and explore different methods for achieving it.\n",
       "\n",
       "This project will provide a comprehensive understanding of knowledge graph completion, self-supervised learning, and multimodal data processing. It will also equip students with the skills to develop innovative solutions for real-world problems. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(synopsis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Project Title:  Knowledge-Enhanced Multimodal Graph Attention Network for Personalized Recommendation\n",
       "\n",
       "## Paper Title:  KGAT: Knowledge Graph Attention Network for Recommendation\n",
       "\n",
       "## Abstract: \n",
       "\n",
       "This project aims to enhance personalized recommendation systems by leveraging the power of knowledge graphs and multimodal data. We propose a novel Knowledge-Enhanced Multimodal Graph Attention Network (KEM-GAT) that integrates knowledge graph embeddings with visual and textual information to provide more accurate and explainable recommendations. KEM-GAT utilizes a multi-head attention mechanism to capture complex relationships between users, items, and knowledge entities, leading to improved recommendation performance.\n",
       "\n",
       "## Motivation:\n",
       "\n",
       "* **Limitations of Traditional Recommender Systems:** Existing recommendation systems primarily rely on user-item interactions, neglecting valuable contextual information from knowledge graphs and multimodal data.\n",
       "* **Knowledge Graph Integration:** Incorporating knowledge graphs can enhance recommendation accuracy by providing rich semantic relationships between entities, leading to better understanding of user preferences and item attributes.\n",
       "* **Multimodal Data Fusion:** Combining visual and textual information can provide a more comprehensive representation of items, leading to more relevant and engaging recommendations.\n",
       "* **Explainable Recommendations:** Our approach aims to provide explainable recommendations by highlighting the knowledge entities and multimodal features that influence the recommendation process, fostering user trust and understanding.\n",
       "\n",
       "## Objectives:\n",
       "\n",
       "* **Develop a novel KEM-GAT architecture:** This architecture will integrate knowledge graph embeddings, visual features (e.g., images), and textual features (e.g., descriptions) into a unified graph representation.\n",
       "* **Implement a multi-head attention mechanism:** This mechanism will allow the model to capture complex relationships between users, items, and knowledge entities, leading to more accurate and personalized recommendations.\n",
       "* **Evaluate the performance of KEM-GAT on real-world recommendation datasets:** We will compare KEM-GAT's performance against existing recommendation methods, demonstrating its effectiveness in capturing user preferences and providing relevant recommendations.\n",
       "* **Develop a visualization tool to analyze the influence of knowledge entities and multimodal features on recommendations:** This tool will provide insights into the decision-making process of the model, enhancing its explainability and transparency.\n",
       "\n",
       "## Architecture:\n",
       "\n",
       "1. **Knowledge Graph Embedding:** Embed knowledge entities from a knowledge graph into a low-dimensional vector space.\n",
       "2. **Multimodal Feature Extraction:** Extract visual features from images and textual features from item descriptions using pre-trained models.\n",
       "3. **Multimodal Graph Construction:** Combine user, item, knowledge entity, and multimodal feature nodes into a heterogeneous graph, connecting them based on their relationships.\n",
       "4. **KEM-GAT Layer:** Apply a multi-head attention mechanism to learn complex relationships between nodes within the multimodal graph.\n",
       "5. **Recommendation Generation:** Predict user preferences and generate personalized recommendations based on the learned representations.\n",
       "\n",
       "## Application Areas:\n",
       "\n",
       "* **E-commerce:** Recommending products based on user preferences, purchase history, and product information from a knowledge graph.\n",
       "* **Social Media:** Recommending content based on user interests, social connections, and topic-related knowledge entities.\n",
       "* **News Recommendation:** Recommending news articles based on user profiles, reading history, and current events from a knowledge graph.\n",
       "* **Movie Recommendation:** Recommending movies based on user ratings, genre preferences, and movie information from a knowledge graph.\n",
       "\n",
       "## Conclusion:\n",
       "\n",
       "The KEM-GAT model presents a promising approach to enhance personalized recommendation systems by leveraging the power of knowledge graphs and multimodal data. By integrating knowledge entities and multimodal features into a unified graph representation, KEM-GAT captures complex relationships between users, items, and knowledge entities, leading to more accurate, explainable, and personalized recommendations.\n",
       "\n",
       "## Project Plan:\n",
       "\n",
       "**Month 1:**\n",
       "\n",
       "* **Literature Review:** Study relevant papers on knowledge graph embedding, multimodal data fusion, and graph neural networks.\n",
       "* **Dataset Selection:** Choose a suitable recommendation dataset that includes user-item interactions, knowledge graph information, and multimodal features.\n",
       "* **Knowledge Graph Construction:** Construct a knowledge graph based on the chosen dataset, including relevant entities and relationships.\n",
       "\n",
       "**Month 2:**\n",
       "\n",
       "* **Multimodal Feature Extraction:** Implement pre-trained models to extract visual and textual features from the dataset.\n",
       "* **KEM-GAT Architecture Implementation:** Develop the KEM-GAT architecture, including the multi-head attention mechanism.\n",
       "* **Model Training and Evaluation:** Train and evaluate the KEM-GAT model on the chosen dataset, comparing its performance to existing recommendation methods.\n",
       "\n",
       "**Month 3:**\n",
       "\n",
       "* **Visualization Tool Development:** Create a visualization tool to analyze the influence of knowledge entities and multimodal features on recommendations.\n",
       "* **Project Report Writing:** Document the project findings, including the methodology, results, and discussion.\n",
       "* **Project Presentation:** Prepare and present the project results to an audience.\n",
       "\n",
       "## Topics to Study:\n",
       "\n",
       "* **Knowledge Graph Embedding:** Techniques for representing entities and relationships from a knowledge graph in a low-dimensional vector space.\n",
       "* **Multimodal Data Fusion:** Methods for combining visual, textual, and other modalities of data for improved representation and understanding.\n",
       "* **Graph Neural Networks:** Architectures and algorithms for learning from graph-structured data.\n",
       "* **Attention Mechanisms:** Techniques for selectively focusing on relevant information within a dataset.\n",
       "* **Recommendation Systems:** Principles and techniques for personalized recommendation systems.\n",
       "\n",
       "## Project Names:\n",
       "\n",
       "* Knowledge-Enhanced Multimodal Recommendations\n",
       "* Graph-Powered Personalized Recommendations\n",
       "* Multimodal Graph Attention for Recommendation\n",
       "* Explainable Recommendations with Knowledge Graphs\n",
       "* Personalized Recommendations with Multimodal Insights \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(llm.invoke(synopsis_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Project Title:  Hyperbolic Knowledge Graph Embedding for Enhanced Recommendation Systems\n",
       "\n",
       "## Paper Title: Hyperbolic Graph Attention Network (HGAT)\n",
       "\n",
       "## Abstract: \n",
       "\n",
       "This project proposes an enhanced knowledge graph embedding (KGE) model for recommendation systems based on the Hyperbolic Graph Attention Network (HGAT). HGAT leverages the inherent hierarchical structure of knowledge graphs by embedding entities and relationships in hyperbolic space, allowing for more efficient representation of complex relationships and improved recommendation accuracy. We aim to address the limitations of existing KGE methods in handling large, complex knowledge graphs and explore the potential of HGAT for personalized and explainable recommendations.\n",
       "\n",
       "## Motivation:\n",
       "\n",
       "* **Scalability:** Existing KGE methods struggle to scale to large, complex knowledge graphs, limiting their effectiveness in real-world applications.\n",
       "* **Explanatory Power:** Many recommendation systems lack transparency, making it difficult to understand the reasoning behind recommendations.\n",
       "* **Hierarchical Representation:** Knowledge graphs exhibit inherent hierarchical structures, which are not effectively captured by traditional Euclidean embedding spaces.\n",
       "* **Improved Recommendation Accuracy:** Hyperbolic space provides a more natural representation for hierarchical data, potentially leading to better performance in recommendation tasks.\n",
       "\n",
       "## Objectives:\n",
       "\n",
       "* **Develop a novel HGAT-based KGE model:**  We aim to design an optimized HGAT architecture specifically tailored for KGE and recommendation tasks.\n",
       "* **Enhance recommendation accuracy:** We will evaluate the performance of our HGAT-based model on standard recommendation datasets and compare it with existing KGE methods.\n",
       "* **Improve explainability of recommendations:** We will explore techniques to interpret the predictions made by our model, providing insights into the reasoning behind recommendations.\n",
       "* **Investigate the impact of different hyperbolic geometries:** We will experiment with different hyperbolic geometries and analyze their effect on model performance and explainability.\n",
       "\n",
       "## Architecture:\n",
       "\n",
       "* **Knowledge Graph Construction:** Build a knowledge graph from a chosen recommendation dataset.\n",
       "* **Hyperbolic Embedding:** Embed entities and relationships into a hyperbolic space using HGAT.\n",
       "* **Recommendation Generation:** Utilize the embedded knowledge graph to generate recommendations based on user preferences and item characteristics.\n",
       "* **Explainability Module:** Develop a mechanism to interpret the recommendations and provide insights into the reasoning process.\n",
       "\n",
       "## Application Areas:\n",
       "\n",
       "* **E-commerce:** Personalized product recommendations based on user history and item relationships.\n",
       "* **Social Media:**  Personalized content recommendations based on user interests and social connections.\n",
       "* **Movie/Music Recommendation:**  Suggesting movies or music based on user preferences and genre relationships.\n",
       "* **Healthcare:** Recommending treatments or therapies based on patient history and medical knowledge.\n",
       "\n",
       "## Conclusion:\n",
       "\n",
       "This project aims to contribute to the development of more efficient and effective KGE models for recommendation systems. By leveraging the power of HGAT and exploring its potential for explainability, we seek to improve the accuracy and transparency of personalized recommendations.\n",
       "\n",
       "## Project Plan:\n",
       "\n",
       "**Phase 1 (Week 1-2):**\n",
       "* Literature Review:  Deep dive into KGE, HGAT, and recommendation systems.\n",
       "* Dataset Selection: Choose a relevant recommendation dataset and construct a knowledge graph.\n",
       "* Model Architecture: Design the HGAT-based KGE model and develop the necessary components.\n",
       "\n",
       "**Phase 2 (Week 3-4):**\n",
       "* Implementation: Implement the HGAT model using a deep learning framework (e.g., PyTorch).\n",
       "* Training and Evaluation: Train the model on the chosen dataset and evaluate its performance against baseline methods.\n",
       "\n",
       "**Phase 3 (Week 5-6):**\n",
       "* Explainability Module: Design and implement a module to interpret the recommendations and provide insights.\n",
       "* Experimentation: Investigate different hyperbolic geometries and analyze their impact on performance.\n",
       "* Report Writing: Document the project findings, methodology, and contributions.\n",
       "\n",
       "**Phase 4 (Week 7-8):**\n",
       "* Presentation Preparation: Prepare a detailed presentation of the project and its outcomes.\n",
       "* Final Presentation: Present the project to a panel of experts and peers.\n",
       "\n",
       "## Topics to Study:\n",
       "\n",
       "* **Knowledge Representation:**  Understand different knowledge representation techniques, including RDF, OWL, and knowledge graphs.\n",
       "* **Graph Neural Networks:** Learn about various GNN architectures, including GAT, GCN, and their applications.\n",
       "* **Hyperbolic Geometry:**  Explore the basics of hyperbolic geometry and its applications in machine learning.\n",
       "* **Recommendation Systems:**  Study different recommendation algorithms, including collaborative filtering, content-based filtering, and knowledge-based recommendation.\n",
       "* **Explainable AI:**  Learn about techniques for interpreting and explaining the predictions of machine learning models. \n",
       "* **Deep Learning Frameworks:**  Gain proficiency in using a deep learning framework like PyTorch or TensorFlow.\n",
       "\n",
       "This project provides a challenging and rewarding opportunity to explore cutting-edge research in knowledge graph embedding and recommendation systems. By focusing on the novel application of HGAT and addressing real-world challenges, this project has the potential to make a significant impact in the field of personalized AI. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(llm.invoke(synopsis_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Project Title:  KG-LLM: Enhancing Knowledge Graph Reasoning with Large Language Models\n",
       "\n",
       "## Paper Title:  Knowledge Graph Completion Based on Recurrent Neural Network\n",
       "\n",
       "## Abstract: \n",
       "\n",
       "This project aims to leverage the power of Large Language Models (LLMs) to enhance knowledge graph reasoning, specifically focusing on the task of knowledge graph completion. We propose a novel framework that integrates LLMs with existing knowledge graph embedding techniques, enabling more accurate and robust prediction of missing relationships in knowledge graphs. Our approach utilizes the vast knowledge and reasoning abilities of LLMs to generate contextualized representations of entities and relations, which are then used to refine the embeddings learned by traditional knowledge graph embedding models. This fusion of LLM capabilities with graph neural networks addresses the limitations of current methods, particularly their struggle with handling complex relationships and scalability to large knowledge graphs.\n",
       "\n",
       "## Motivation:\n",
       "\n",
       "* **Addressing limitations of current knowledge graph completion methods:** Existing methods often struggle with capturing complex relationships and scaling to large knowledge graphs. \n",
       "* **Leveraging the power of LLMs for improved reasoning:** LLMs possess vast knowledge and reasoning abilities, offering a powerful tool for enhancing knowledge graph completion.\n",
       "* **Bridging the gap between symbolic and neural approaches:**  Our framework combines the strengths of both symbolic knowledge representation (knowledge graphs) and neural reasoning (LLMs) for a more comprehensive approach.\n",
       "* **Enabling more accurate and robust knowledge graph completion:** By incorporating LLM knowledge and reasoning, our approach aims to significantly improve the accuracy and robustness of knowledge graph completion.\n",
       "\n",
       "## Objectives:\n",
       "\n",
       "* **Develop a novel hybrid framework for knowledge graph completion:**  This framework will integrate LLMs with existing knowledge graph embedding techniques.\n",
       "* **Investigate the effectiveness of using LLMs for generating contextualized entity and relation representations:** We will explore various techniques for utilizing LLMs to enrich knowledge graph embeddings.\n",
       "* **Evaluate the performance of our proposed framework on benchmark knowledge graph datasets:** We will compare our approach against state-of-the-art knowledge graph completion methods.\n",
       "* **Explore applications of our enhanced knowledge graph in downstream tasks:**  We will investigate the potential of our framework for improving knowledge-based tasks like question answering and recommendation systems.\n",
       "\n",
       "## Architecture:\n",
       "\n",
       "**Step 1:**  Pre-train an LLM on a large corpus of text and code.\n",
       "**Step 2:**  Utilize the pre-trained LLM to generate contextualized representations of entities and relations within the target knowledge graph.\n",
       "**Step 3:**  Embed these contextualized representations into a vector space using a suitable knowledge graph embedding technique (e.g., TransE, RotatE).\n",
       "**Step 4:**  Train a knowledge graph completion model using the enriched embeddings, enabling it to predict missing relationships in the knowledge graph.\n",
       "\n",
       "## Application Areas:\n",
       "\n",
       "* **Knowledge-based question answering:** Our enhanced knowledge graph can be used to answer complex questions by leveraging the combined knowledge of the graph and the LLM.\n",
       "* **Recommendation systems:** By incorporating knowledge graph reasoning, our approach can improve the accuracy and explainability of recommendations.\n",
       "* **Drug discovery and disease prediction:**  Our enhanced knowledge graph can be used to analyze relationships between drugs, diseases, and biological entities for potential drug discovery and disease prediction applications.\n",
       "* **Natural language understanding:**  Our framework can be used to enhance natural language understanding tasks by providing a richer semantic understanding of text.\n",
       "\n",
       "## Conclusion:\n",
       "\n",
       "This project presents a novel and promising approach to knowledge graph completion by leveraging the capabilities of LLMs. We anticipate that our framework will contribute significantly to the advancement of knowledge representation and reasoning, leading to improvements in various applications across different domains.\n",
       "\n",
       "## Project Plan:\n",
       "\n",
       "**Phase 1 (1 month):**\n",
       "\n",
       "* **Literature review and data collection:**  Study relevant papers on knowledge graph completion, LLMs, and knowledge graph embedding techniques.  Identify and select appropriate benchmark datasets.\n",
       "* **Pre-training an LLM:** Choose a suitable LLM architecture and pre-train it on a large corpus of text and code.\n",
       "* **Development of LLM-based knowledge graph embedding technique:**  Design and implement a method for generating contextualized entity and relation representations using the pre-trained LLM.\n",
       "\n",
       "**Phase 2 (1 month):**\n",
       "\n",
       "* **Implementation of knowledge graph completion model:** Train a knowledge graph completion model using the enriched embeddings generated in Phase 1. \n",
       "* **Evaluation and analysis:** Evaluate the performance of our proposed framework on benchmark datasets and compare it against state-of-the-art methods. Analyze the results to understand strengths and weaknesses of our approach.\n",
       "\n",
       "**Phase 3 (1 month):**\n",
       "\n",
       "* **Exploration of downstream applications:**  Investigate the potential of our enhanced knowledge graph for improving knowledge-based tasks like question answering and recommendation systems.\n",
       "* **Project documentation and presentation:**  Prepare a comprehensive report summarizing our findings, including experimental results, analysis, and future directions.\n",
       "\n",
       "## Topics to Study:\n",
       "\n",
       "**Within 3 weeks, you should familiarize yourself with the following topics:**\n",
       "\n",
       "* **Knowledge Graphs:** Basics of knowledge graphs, their structure, and applications.\n",
       "* **Knowledge Graph Embedding:**  Different knowledge graph embedding techniques (TransE, RotatE, etc.) and their strengths and weaknesses.\n",
       "* **Large Language Models:**  Understanding the architecture and capabilities of LLMs (e.g., BERT, GPT-3).\n",
       "* **Pre-training and Fine-tuning LLMs:**  Methods for pre-training and fine-tuning LLMs for specific tasks.\n",
       "* **Graph Neural Networks:**  Fundamentals of graph neural networks and their applications in knowledge graph completion.\n",
       "* **Evaluation Metrics for Knowledge Graph Completion:**  Understanding relevant metrics for evaluating knowledge graph completion models (e.g., MRR, Hits@N).\n",
       "\n",
       "This project provides a challenging yet rewarding experience that allows you to explore the cutting-edge intersection of LLMs and knowledge graphs. By successfully completing this project, you will gain valuable experience in research, development, and evaluation of innovative AI solutions. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(llm.invoke(synopsis_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Project Title:  Hyperbolic Knowledge Graph Attention Network for Multimodal Recommendation \n",
       "\n",
       "## Paper Title:  Hyperbolic Graph Attention Network \n",
       "\n",
       "## Abstract: \n",
       "\n",
       "This project aims to develop a novel multimodal recommendation system based on a Hyperbolic Knowledge Graph Attention Network (HKGAN).  HKGAN leverages the benefits of hyperbolic geometry to represent entities and relations in a knowledge graph, enhancing the ability to capture complex relationships and hierarchies within the data. By integrating multimodal information, such as text, images, and user preferences, HKGAN can provide more accurate and personalized recommendations. \n",
       "\n",
       "## Motivation:\n",
       "\n",
       "* **Limitations of Euclidean Embeddings:** Traditional knowledge graph embedding methods using Euclidean space struggle to represent hierarchical structures and complex relationships between entities.\n",
       "* **Hyperbolic Geometry for Enhanced Representation:** Hyperbolic geometry offers a more suitable space for modeling hierarchical structures and long-range dependencies, leading to more accurate embeddings.\n",
       "* **Multimodal Information for Richer Context:** Incorporating multimodal information, such as images and text, provides a richer context for understanding user preferences and making more relevant recommendations.\n",
       "* **Personalized and Explainable Recommendations:** HKGAN aims to provide personalized recommendations with explanations based on the learned relationships within the knowledge graph, improving user trust and satisfaction.\n",
       "\n",
       "## Objectives: \n",
       "\n",
       "* **Developing a novel HKGAN architecture:** Design a specialized architecture that combines hyperbolic graph attention networks with multimodal information integration.\n",
       "* **Hyperbolic Knowledge Graph Embedding:** Develop a method for embedding entities and relations in a hyperbolic space, capturing hierarchical structures and complex relationships.\n",
       "* **Multimodal Information Fusion:** Integrate multimodal data sources, such as text, images, and user profiles, into the HKGAN architecture to enhance recommendation accuracy.\n",
       "* **Personalized and Explainable Recommendations:**  Generate personalized recommendations with explainable reasoning based on the learned relationships in the knowledge graph.\n",
       "\n",
       "## Architecture:\n",
       "\n",
       "1. **Multimodal Feature Extraction:** Extract features from different modalities (text, images, user profiles) using appropriate pre-trained models.\n",
       "2. **Hyperbolic Knowledge Graph Embedding:** Embed entities and relations in a hyperbolic space using a specialized embedding method.\n",
       "3. **Hyperbolic Graph Attention Network:** Apply a hyperbolic graph attention network to learn relationships and preferences based on multimodal features and embedded entities.\n",
       "4. **Recommendation Generation:** Generate personalized recommendations based on the learned relationships and preferences.\n",
       "5. **Explanation Module:** Provide explanations for recommendations based on the learned relationships and multimodal information.\n",
       "\n",
       "## Application Areas: \n",
       "\n",
       "* **E-commerce Recommendation:** Recommending products based on user preferences, product descriptions, and images.\n",
       "* **Movie/Music Recommendation:** Recommending movies or music based on user preferences, genre information, and movie posters/album covers.\n",
       "* **Social Media Recommendation:** Recommending relevant content and connections based on user interactions, social network graphs, and profile information.\n",
       "* **Knowledge Graph Completion:** Predicting missing relationships in knowledge graphs based on multimodal information and learned embeddings.\n",
       "\n",
       "## Conclusion:\n",
       "\n",
       "This project aims to develop a novel HKGAN-based multimodal recommendation system that leverages the advantages of hyperbolic geometry and multimodal information integration. The project is expected to achieve high accuracy, personalization, and explainability in recommendation tasks, contributing significantly to the field of knowledge graph-based recommendation systems.\n",
       "\n",
       "## Project Plan:\n",
       "\n",
       "**Phase 1 (Weeks 1-2):**\n",
       "\n",
       "* **Literature Review:** Conduct a comprehensive review of existing knowledge graph embedding methods, hyperbolic geometry, and multimodal information integration techniques.\n",
       "* **Dataset Selection:** Choose a suitable multimodal dataset for recommendation tasks (e.g., Amazon product dataset, MovieLens dataset).\n",
       "* **Feature Extraction:** Develop methods for extracting features from different modalities (text, images, user profiles).\n",
       "\n",
       "**Phase 2 (Weeks 3-4):**\n",
       "\n",
       "* **Hyperbolic Embedding:** Implement a hyperbolic embedding method for entities and relations in the chosen knowledge graph.\n",
       "* **HKGAN Architecture:** Design and implement the HKGAN architecture incorporating hyperbolic attention mechanisms and multimodal information fusion.\n",
       "* **Training and Evaluation:** Train the HKGAN model on the chosen dataset and evaluate its performance using relevant metrics (e.g., accuracy, precision, recall).\n",
       "\n",
       "**Phase 3 (Weeks 5-6):**\n",
       "\n",
       "* **Explainability Module:** Develop an explainability module to provide insights into the generated recommendations.\n",
       "* **Application Development:** Integrate the HKGAN model into a user-friendly application for real-world recommendation scenarios.\n",
       "* **Final Report and Presentation:** Prepare a comprehensive report and presentation summarizing the project findings and contributions.\n",
       "\n",
       "## Topics to Study:\n",
       "\n",
       "* **Knowledge Representation:** Concepts of knowledge graphs, entity-relationship modeling, and knowledge graph embedding methods.\n",
       "* **Graph Neural Networks:** Basic concepts, architectures, and applications of GNNs.\n",
       "* **Hyperbolic Geometry:** Introduction to hyperbolic geometry, its properties, and its applications in machine learning.\n",
       "* **Multimodal Information Fusion:** Techniques for integrating and processing data from multiple modalities.\n",
       "* **Recommendation Systems:** Principles of recommendation systems, evaluation metrics, and common techniques.\n",
       "\n",
       "**Note:** This project plan provides a general framework. The specific details and tasks may vary depending on the chosen dataset, chosen embedding method, and the specific focus of the project.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(llm.invoke(synopsis_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
